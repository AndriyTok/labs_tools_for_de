{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import keras\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior() \n",
    "\n",
    "from sklearn import datasets\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Набір даних"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PrepareData():\n",
    "    data = datasets.make_regression()\n",
    "    df = pd.DataFrame(data[0], columns=[f\"feature_{i+1}\" for i in range(data[0].shape[1])])\n",
    "    df[\"target\"] = data[1]\n",
    "    \n",
    "    x=df.iloc[: , :-1]\n",
    "    y=df.iloc[: , -1]\n",
    "    \n",
    "    return df, x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, x , y = PrepareData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>feature_6</th>\n",
       "      <th>feature_7</th>\n",
       "      <th>feature_8</th>\n",
       "      <th>feature_9</th>\n",
       "      <th>feature_10</th>\n",
       "      <th>...</th>\n",
       "      <th>feature_92</th>\n",
       "      <th>feature_93</th>\n",
       "      <th>feature_94</th>\n",
       "      <th>feature_95</th>\n",
       "      <th>feature_96</th>\n",
       "      <th>feature_97</th>\n",
       "      <th>feature_98</th>\n",
       "      <th>feature_99</th>\n",
       "      <th>feature_100</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.458309</td>\n",
       "      <td>-1.414563</td>\n",
       "      <td>-0.119349</td>\n",
       "      <td>0.985096</td>\n",
       "      <td>-0.229066</td>\n",
       "      <td>2.038558</td>\n",
       "      <td>0.660387</td>\n",
       "      <td>-0.246422</td>\n",
       "      <td>-0.134572</td>\n",
       "      <td>0.053132</td>\n",
       "      <td>...</td>\n",
       "      <td>1.306065</td>\n",
       "      <td>-0.121804</td>\n",
       "      <td>-0.650649</td>\n",
       "      <td>-0.674952</td>\n",
       "      <td>-0.945565</td>\n",
       "      <td>-1.526177</td>\n",
       "      <td>1.322471</td>\n",
       "      <td>0.263927</td>\n",
       "      <td>0.138195</td>\n",
       "      <td>-146.752976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.184568</td>\n",
       "      <td>0.051398</td>\n",
       "      <td>0.297456</td>\n",
       "      <td>-0.696543</td>\n",
       "      <td>0.429655</td>\n",
       "      <td>1.177929</td>\n",
       "      <td>-1.245496</td>\n",
       "      <td>0.561334</td>\n",
       "      <td>-1.068472</td>\n",
       "      <td>1.142458</td>\n",
       "      <td>...</td>\n",
       "      <td>0.865929</td>\n",
       "      <td>0.735658</td>\n",
       "      <td>-1.406908</td>\n",
       "      <td>-0.667031</td>\n",
       "      <td>-0.980006</td>\n",
       "      <td>0.960386</td>\n",
       "      <td>-0.385922</td>\n",
       "      <td>0.348125</td>\n",
       "      <td>-1.134025</td>\n",
       "      <td>-251.696141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-2.212705</td>\n",
       "      <td>-1.650461</td>\n",
       "      <td>-0.656921</td>\n",
       "      <td>0.341239</td>\n",
       "      <td>1.254488</td>\n",
       "      <td>-1.003174</td>\n",
       "      <td>-0.737841</td>\n",
       "      <td>-0.519014</td>\n",
       "      <td>0.415315</td>\n",
       "      <td>0.276040</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020196</td>\n",
       "      <td>-0.442832</td>\n",
       "      <td>1.760474</td>\n",
       "      <td>-0.310738</td>\n",
       "      <td>0.444769</td>\n",
       "      <td>1.067164</td>\n",
       "      <td>-0.808231</td>\n",
       "      <td>0.633832</td>\n",
       "      <td>0.432363</td>\n",
       "      <td>421.583120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.238575</td>\n",
       "      <td>0.839977</td>\n",
       "      <td>0.482139</td>\n",
       "      <td>0.408026</td>\n",
       "      <td>0.013249</td>\n",
       "      <td>-1.681527</td>\n",
       "      <td>-1.164910</td>\n",
       "      <td>-0.033822</td>\n",
       "      <td>-0.220416</td>\n",
       "      <td>-0.020076</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.160685</td>\n",
       "      <td>2.046046</td>\n",
       "      <td>0.159493</td>\n",
       "      <td>-0.026297</td>\n",
       "      <td>-0.323021</td>\n",
       "      <td>-1.024351</td>\n",
       "      <td>0.959392</td>\n",
       "      <td>-0.765704</td>\n",
       "      <td>0.474683</td>\n",
       "      <td>-8.291261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.008220</td>\n",
       "      <td>-0.423903</td>\n",
       "      <td>1.327341</td>\n",
       "      <td>1.185998</td>\n",
       "      <td>1.317878</td>\n",
       "      <td>-1.666737</td>\n",
       "      <td>-0.910308</td>\n",
       "      <td>-0.227997</td>\n",
       "      <td>-0.309544</td>\n",
       "      <td>-0.876813</td>\n",
       "      <td>...</td>\n",
       "      <td>0.740073</td>\n",
       "      <td>-0.801801</td>\n",
       "      <td>-1.252117</td>\n",
       "      <td>1.757459</td>\n",
       "      <td>0.079407</td>\n",
       "      <td>-1.127582</td>\n",
       "      <td>0.369045</td>\n",
       "      <td>2.341907</td>\n",
       "      <td>-0.828913</td>\n",
       "      <td>88.625874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>-0.713689</td>\n",
       "      <td>1.058993</td>\n",
       "      <td>0.624049</td>\n",
       "      <td>1.614539</td>\n",
       "      <td>-2.224568</td>\n",
       "      <td>0.154761</td>\n",
       "      <td>0.192608</td>\n",
       "      <td>-2.644213</td>\n",
       "      <td>0.758390</td>\n",
       "      <td>1.037912</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.208973</td>\n",
       "      <td>-0.498010</td>\n",
       "      <td>-1.312505</td>\n",
       "      <td>0.112872</td>\n",
       "      <td>0.734496</td>\n",
       "      <td>-0.574533</td>\n",
       "      <td>1.732499</td>\n",
       "      <td>1.278275</td>\n",
       "      <td>-0.255335</td>\n",
       "      <td>-51.167648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.515322</td>\n",
       "      <td>-0.936743</td>\n",
       "      <td>0.900485</td>\n",
       "      <td>0.442852</td>\n",
       "      <td>-1.769131</td>\n",
       "      <td>-0.032845</td>\n",
       "      <td>-0.661873</td>\n",
       "      <td>-0.168311</td>\n",
       "      <td>0.604524</td>\n",
       "      <td>0.369986</td>\n",
       "      <td>...</td>\n",
       "      <td>1.421815</td>\n",
       "      <td>0.907686</td>\n",
       "      <td>-2.182003</td>\n",
       "      <td>-1.628828</td>\n",
       "      <td>1.202632</td>\n",
       "      <td>-0.994949</td>\n",
       "      <td>-0.448098</td>\n",
       "      <td>0.245865</td>\n",
       "      <td>1.207567</td>\n",
       "      <td>-232.356306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>-0.267101</td>\n",
       "      <td>1.632658</td>\n",
       "      <td>0.403116</td>\n",
       "      <td>0.406072</td>\n",
       "      <td>-1.217460</td>\n",
       "      <td>-2.257879</td>\n",
       "      <td>1.021372</td>\n",
       "      <td>0.133101</td>\n",
       "      <td>-0.927658</td>\n",
       "      <td>-0.601537</td>\n",
       "      <td>...</td>\n",
       "      <td>0.490868</td>\n",
       "      <td>-0.334489</td>\n",
       "      <td>-0.106549</td>\n",
       "      <td>-0.570443</td>\n",
       "      <td>-0.736798</td>\n",
       "      <td>-0.714398</td>\n",
       "      <td>-0.035845</td>\n",
       "      <td>-0.986388</td>\n",
       "      <td>-0.565732</td>\n",
       "      <td>51.896355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>-0.433523</td>\n",
       "      <td>0.358092</td>\n",
       "      <td>0.738480</td>\n",
       "      <td>-0.126734</td>\n",
       "      <td>1.044739</td>\n",
       "      <td>-0.102726</td>\n",
       "      <td>0.616420</td>\n",
       "      <td>1.723388</td>\n",
       "      <td>0.371396</td>\n",
       "      <td>-1.719797</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.721579</td>\n",
       "      <td>-1.267375</td>\n",
       "      <td>-0.573148</td>\n",
       "      <td>0.617204</td>\n",
       "      <td>-0.032316</td>\n",
       "      <td>-2.695158</td>\n",
       "      <td>1.708088</td>\n",
       "      <td>-0.184432</td>\n",
       "      <td>1.346812</td>\n",
       "      <td>-207.799016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>-2.953206</td>\n",
       "      <td>-0.613831</td>\n",
       "      <td>1.597998</td>\n",
       "      <td>0.439559</td>\n",
       "      <td>-0.037202</td>\n",
       "      <td>0.446060</td>\n",
       "      <td>-0.844268</td>\n",
       "      <td>0.933229</td>\n",
       "      <td>0.805330</td>\n",
       "      <td>-0.343354</td>\n",
       "      <td>...</td>\n",
       "      <td>0.334979</td>\n",
       "      <td>1.477355</td>\n",
       "      <td>-0.825535</td>\n",
       "      <td>-2.263402</td>\n",
       "      <td>1.069569</td>\n",
       "      <td>-0.482113</td>\n",
       "      <td>1.445163</td>\n",
       "      <td>0.118803</td>\n",
       "      <td>-0.364368</td>\n",
       "      <td>-42.468183</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 101 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    feature_1  feature_2  feature_3  feature_4  feature_5  feature_6  \\\n",
       "0    0.458309  -1.414563  -0.119349   0.985096  -0.229066   2.038558   \n",
       "1    0.184568   0.051398   0.297456  -0.696543   0.429655   1.177929   \n",
       "2   -2.212705  -1.650461  -0.656921   0.341239   1.254488  -1.003174   \n",
       "3    0.238575   0.839977   0.482139   0.408026   0.013249  -1.681527   \n",
       "4    1.008220  -0.423903   1.327341   1.185998   1.317878  -1.666737   \n",
       "..        ...        ...        ...        ...        ...        ...   \n",
       "95  -0.713689   1.058993   0.624049   1.614539  -2.224568   0.154761   \n",
       "96   0.515322  -0.936743   0.900485   0.442852  -1.769131  -0.032845   \n",
       "97  -0.267101   1.632658   0.403116   0.406072  -1.217460  -2.257879   \n",
       "98  -0.433523   0.358092   0.738480  -0.126734   1.044739  -0.102726   \n",
       "99  -2.953206  -0.613831   1.597998   0.439559  -0.037202   0.446060   \n",
       "\n",
       "    feature_7  feature_8  feature_9  feature_10  ...  feature_92  feature_93  \\\n",
       "0    0.660387  -0.246422  -0.134572    0.053132  ...    1.306065   -0.121804   \n",
       "1   -1.245496   0.561334  -1.068472    1.142458  ...    0.865929    0.735658   \n",
       "2   -0.737841  -0.519014   0.415315    0.276040  ...    0.020196   -0.442832   \n",
       "3   -1.164910  -0.033822  -0.220416   -0.020076  ...   -1.160685    2.046046   \n",
       "4   -0.910308  -0.227997  -0.309544   -0.876813  ...    0.740073   -0.801801   \n",
       "..        ...        ...        ...         ...  ...         ...         ...   \n",
       "95   0.192608  -2.644213   0.758390    1.037912  ...   -0.208973   -0.498010   \n",
       "96  -0.661873  -0.168311   0.604524    0.369986  ...    1.421815    0.907686   \n",
       "97   1.021372   0.133101  -0.927658   -0.601537  ...    0.490868   -0.334489   \n",
       "98   0.616420   1.723388   0.371396   -1.719797  ...   -1.721579   -1.267375   \n",
       "99  -0.844268   0.933229   0.805330   -0.343354  ...    0.334979    1.477355   \n",
       "\n",
       "    feature_94  feature_95  feature_96  feature_97  feature_98  feature_99  \\\n",
       "0    -0.650649   -0.674952   -0.945565   -1.526177    1.322471    0.263927   \n",
       "1    -1.406908   -0.667031   -0.980006    0.960386   -0.385922    0.348125   \n",
       "2     1.760474   -0.310738    0.444769    1.067164   -0.808231    0.633832   \n",
       "3     0.159493   -0.026297   -0.323021   -1.024351    0.959392   -0.765704   \n",
       "4    -1.252117    1.757459    0.079407   -1.127582    0.369045    2.341907   \n",
       "..         ...         ...         ...         ...         ...         ...   \n",
       "95   -1.312505    0.112872    0.734496   -0.574533    1.732499    1.278275   \n",
       "96   -2.182003   -1.628828    1.202632   -0.994949   -0.448098    0.245865   \n",
       "97   -0.106549   -0.570443   -0.736798   -0.714398   -0.035845   -0.986388   \n",
       "98   -0.573148    0.617204   -0.032316   -2.695158    1.708088   -0.184432   \n",
       "99   -0.825535   -2.263402    1.069569   -0.482113    1.445163    0.118803   \n",
       "\n",
       "    feature_100      target  \n",
       "0      0.138195 -146.752976  \n",
       "1     -1.134025 -251.696141  \n",
       "2      0.432363  421.583120  \n",
       "3      0.474683   -8.291261  \n",
       "4     -0.828913   88.625874  \n",
       "..          ...         ...  \n",
       "95    -0.255335  -51.167648  \n",
       "96     1.207567 -232.356306  \n",
       "97    -0.565732   51.896355  \n",
       "98     1.346812 -207.799016  \n",
       "99    -0.364368  -42.468183  \n",
       "\n",
       "[100 rows x 101 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size = 0.1, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    keras.layers.Input(x.shape[1]),\n",
    "    keras.layers.Dense(10, activation='relu'),\n",
    "    keras.layers.Dense(5, activation='relu'),\n",
    "    keras.layers.Dense(3, activation='relu'),\n",
    "    keras.layers.Dense(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='mean_squared_error', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 80 samples, validate on 20 samples\n",
      "Epoch 1/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80/80 [==============================] - 0s 2ms/sample - loss: 22276.0393 - val_loss: 16782.7148\n",
      "Epoch 2/1000\n",
      "80/80 [==============================] - 0s 28us/sample - loss: 22270.6062 - val_loss: 16781.3516\n",
      "Epoch 3/1000\n",
      "80/80 [==============================] - 0s 23us/sample - loss: 22265.1462 - val_loss: 16780.0625\n",
      "Epoch 4/1000\n",
      "80/80 [==============================] - 0s 23us/sample - loss: 22260.4893 - val_loss: 16778.8496\n",
      "Epoch 5/1000\n",
      "80/80 [==============================] - 0s 21us/sample - loss: 22256.8811 - val_loss: 16777.5410\n",
      "Epoch 6/1000\n",
      "80/80 [==============================] - 0s 24us/sample - loss: 22252.3135 - val_loss: 16776.3477\n",
      "Epoch 7/1000\n",
      "80/80 [==============================] - 0s 105us/sample - loss: 22248.4043 - val_loss: 16775.2227\n",
      "Epoch 8/1000\n",
      "80/80 [==============================] - 0s 170us/sample - loss: 22244.1086 - val_loss: 16773.9121\n",
      "Epoch 9/1000\n",
      "80/80 [==============================] - 0s 25us/sample - loss: 22241.2122 - val_loss: 16772.3926\n",
      "Epoch 10/1000\n",
      "80/80 [==============================] - 0s 86us/sample - loss: 22237.9902 - val_loss: 16770.7656\n",
      "Epoch 11/1000\n",
      "80/80 [==============================] - 0s 28us/sample - loss: 22234.0852 - val_loss: 16768.9590\n",
      "Epoch 12/1000\n",
      "80/80 [==============================] - 0s 36us/sample - loss: 22230.7598 - val_loss: 16767.1348\n",
      "Epoch 13/1000\n",
      "80/80 [==============================] - 0s 33us/sample - loss: 22227.5247 - val_loss: 16765.4336\n",
      "Epoch 14/1000\n",
      "80/80 [==============================] - 0s 25us/sample - loss: 22223.9055 - val_loss: 16763.9004\n",
      "Epoch 15/1000\n",
      "80/80 [==============================] - 0s 20us/sample - loss: 22220.7644 - val_loss: 16762.4883\n",
      "Epoch 16/1000\n",
      "80/80 [==============================] - 0s 24us/sample - loss: 22216.9259 - val_loss: 16761.0039\n",
      "Epoch 17/1000\n",
      "80/80 [==============================] - 0s 22us/sample - loss: 22213.4226 - val_loss: 16759.2305\n",
      "Epoch 18/1000\n",
      "50/80 [=================>............] - ETA: 0s - loss: 23998.0332"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-31 09:50:46.453053: W tensorflow/c/c_api.cc:305] Operation '{name:'training/Adam/dense_3/bias/v/Assign' id:294 op device:{requested: '', assigned: ''} def:{{{node training/Adam/dense_3/bias/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training/Adam/dense_3/bias/v, training/Adam/dense_3/bias/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2024-01-31 09:50:46.518802: W tensorflow/c/c_api.cc:305] Operation '{name:'training_2/Adam/dense_5/kernel/v/Assign' id:888 op device:{requested: '', assigned: ''} def:{{{node training_2/Adam/dense_5/kernel/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_2/Adam/dense_5/kernel/v, training_2/Adam/dense_5/kernel/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "/Users/mykola/anaconda3/lib/python3.11/site-packages/keras/src/engine/training_v1.py:2335: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates = self.state_updates\n",
      "2024-01-31 09:50:46.570718: W tensorflow/c/c_api.cc:305] Operation '{name:'loss_1/mul' id:724 op device:{requested: '', assigned: ''} def:{{{node loss_1/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_1/mul/x, loss_1/dense_7_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80/80 [==============================] - 0s 266us/sample - loss: 22209.5454 - val_loss: 16757.3145\n",
      "Epoch 19/1000\n",
      "80/80 [==============================] - 0s 43us/sample - loss: 22205.7847 - val_loss: 16755.3457\n",
      "Epoch 20/1000\n",
      "80/80 [==============================] - 0s 40us/sample - loss: 22200.9771 - val_loss: 16753.4004\n",
      "Epoch 21/1000\n",
      "80/80 [==============================] - 0s 57us/sample - loss: 22196.1252 - val_loss: 16751.4531\n",
      "Epoch 22/1000\n",
      "80/80 [==============================] - 0s 27us/sample - loss: 22192.0535 - val_loss: 16749.2695\n",
      "Epoch 23/1000\n",
      "80/80 [==============================] - 0s 22us/sample - loss: 22187.0583 - val_loss: 16747.0195\n",
      "Epoch 24/1000\n",
      "80/80 [==============================] - 0s 24us/sample - loss: 22182.0406 - val_loss: 16744.5410\n",
      "Epoch 25/1000\n",
      "80/80 [==============================] - 0s 21us/sample - loss: 22176.6780 - val_loss: 16741.5527\n",
      "Epoch 26/1000\n",
      "80/80 [==============================] - 0s 29us/sample - loss: 22171.2363 - val_loss: 16738.4727\n",
      "Epoch 27/1000\n",
      "80/80 [==============================] - 0s 26us/sample - loss: 22164.6055 - val_loss: 16735.4668\n",
      "Epoch 28/1000\n",
      "80/80 [==============================] - 0s 20us/sample - loss: 22159.8022 - val_loss: 16732.3125\n",
      "Epoch 29/1000\n",
      "80/80 [==============================] - 0s 100us/sample - loss: 22154.3281 - val_loss: 16729.1836\n",
      "Epoch 30/1000\n",
      "80/80 [==============================] - 0s 23us/sample - loss: 22147.8267 - val_loss: 16726.0410\n",
      "Epoch 31/1000\n",
      "80/80 [==============================] - 0s 53us/sample - loss: 22141.6819 - val_loss: 16722.9629\n",
      "Epoch 32/1000\n",
      "80/80 [==============================] - 0s 35us/sample - loss: 22135.1831 - val_loss: 16719.8477\n",
      "Epoch 33/1000\n",
      "80/80 [==============================] - 0s 24us/sample - loss: 22130.6116 - val_loss: 16716.5098\n",
      "Epoch 34/1000\n",
      "80/80 [==============================] - 0s 27us/sample - loss: 22125.1155 - val_loss: 16713.2754\n",
      "Epoch 35/1000\n",
      "80/80 [==============================] - 0s 23us/sample - loss: 22118.9114 - val_loss: 16710.1328\n",
      "Epoch 36/1000\n",
      "80/80 [==============================] - 0s 25us/sample - loss: 22112.9624 - val_loss: 16707.0000\n",
      "Epoch 37/1000\n",
      "80/80 [==============================] - 0s 19us/sample - loss: 22106.1582 - val_loss: 16703.2441\n",
      "Epoch 38/1000\n",
      "80/80 [==============================] - 0s 25us/sample - loss: 22099.8977 - val_loss: 16699.1152\n",
      "Epoch 39/1000\n",
      "80/80 [==============================] - 0s 17us/sample - loss: 22093.7830 - val_loss: 16694.8906\n",
      "Epoch 40/1000\n",
      "80/80 [==============================] - 0s 19us/sample - loss: 22086.5791 - val_loss: 16690.3398\n",
      "Epoch 41/1000\n",
      "80/80 [==============================] - 0s 21us/sample - loss: 22079.7321 - val_loss: 16685.8066\n",
      "Epoch 42/1000\n",
      "80/80 [==============================] - 0s 21us/sample - loss: 22072.3208 - val_loss: 16681.2148\n",
      "Epoch 43/1000\n",
      "80/80 [==============================] - 0s 18us/sample - loss: 22063.8467 - val_loss: 16676.5352\n",
      "Epoch 44/1000\n",
      "80/80 [==============================] - 0s 19us/sample - loss: 22057.1057 - val_loss: 16671.5117\n",
      "Epoch 45/1000\n",
      "80/80 [==============================] - 0s 17us/sample - loss: 22049.1191 - val_loss: 16665.5801\n",
      "Epoch 46/1000\n",
      "80/80 [==============================] - 0s 20us/sample - loss: 22039.5691 - val_loss: 16659.4180\n",
      "Epoch 47/1000\n",
      "80/80 [==============================] - 0s 25us/sample - loss: 22030.7429 - val_loss: 16652.6797\n",
      "Epoch 48/1000\n",
      "80/80 [==============================] - 0s 18us/sample - loss: 22021.4165 - val_loss: 16645.3320\n",
      "Epoch 49/1000\n",
      "80/80 [==============================] - 0s 21us/sample - loss: 22010.4868 - val_loss: 16637.7852\n",
      "Epoch 50/1000\n",
      "80/80 [==============================] - 0s 56us/sample - loss: 21998.5854 - val_loss: 16629.9512\n",
      "Epoch 51/1000\n",
      "80/80 [==============================] - 0s 106us/sample - loss: 21986.8186 - val_loss: 16621.7715\n",
      "Epoch 52/1000\n",
      "80/80 [==============================] - 0s 68us/sample - loss: 21974.1907 - val_loss: 16613.2871\n",
      "Epoch 53/1000\n",
      "80/80 [==============================] - 0s 55us/sample - loss: 21961.6206 - val_loss: 16604.2695\n",
      "Epoch 54/1000\n",
      "80/80 [==============================] - 0s 28us/sample - loss: 21947.8730 - val_loss: 16595.0938\n",
      "Epoch 55/1000\n",
      "80/80 [==============================] - 0s 19us/sample - loss: 21934.1924 - val_loss: 16585.9590\n",
      "Epoch 56/1000\n",
      "80/80 [==============================] - 0s 17us/sample - loss: 21920.3972 - val_loss: 16576.3965\n",
      "Epoch 57/1000\n",
      "80/80 [==============================] - 0s 51us/sample - loss: 21906.6401 - val_loss: 16566.3008\n",
      "Epoch 58/1000\n",
      "80/80 [==============================] - 0s 32us/sample - loss: 21891.8879 - val_loss: 16556.2012\n",
      "Epoch 59/1000\n",
      "80/80 [==============================] - 0s 22us/sample - loss: 21873.9919 - val_loss: 16546.1250\n",
      "Epoch 60/1000\n",
      "80/80 [==============================] - 0s 29us/sample - loss: 21857.7505 - val_loss: 16535.4746\n",
      "Epoch 61/1000\n",
      "80/80 [==============================] - 0s 19us/sample - loss: 21841.0139 - val_loss: 16524.2402\n",
      "Epoch 62/1000\n",
      "80/80 [==============================] - 0s 22us/sample - loss: 21824.4939 - val_loss: 16512.1602\n",
      "Epoch 63/1000\n",
      "80/80 [==============================] - 0s 25us/sample - loss: 21803.9802 - val_loss: 16500.2402\n",
      "Epoch 64/1000\n",
      "80/80 [==============================] - 0s 18us/sample - loss: 21785.7273 - val_loss: 16488.1992\n",
      "Epoch 65/1000\n",
      "80/80 [==============================] - 0s 16us/sample - loss: 21765.4670 - val_loss: 16475.8730\n",
      "Epoch 66/1000\n",
      "80/80 [==============================] - 0s 15us/sample - loss: 21745.1067 - val_loss: 16463.0352\n",
      "Epoch 67/1000\n",
      "80/80 [==============================] - 0s 14us/sample - loss: 21724.3994 - val_loss: 16449.6934\n",
      "Epoch 68/1000\n",
      "80/80 [==============================] - 0s 15us/sample - loss: 21703.5994 - val_loss: 16435.9102\n",
      "Epoch 69/1000\n",
      "80/80 [==============================] - 0s 14us/sample - loss: 21683.2161 - val_loss: 16422.2324\n",
      "Epoch 70/1000\n",
      "80/80 [==============================] - 0s 15us/sample - loss: 21658.6411 - val_loss: 16408.5820\n",
      "Epoch 71/1000\n",
      "80/80 [==============================] - 0s 15us/sample - loss: 21632.8541 - val_loss: 16395.0195\n",
      "Epoch 72/1000\n",
      "80/80 [==============================] - 0s 14us/sample - loss: 21611.1221 - val_loss: 16380.7373\n",
      "Epoch 73/1000\n",
      "80/80 [==============================] - 0s 16us/sample - loss: 21586.5347 - val_loss: 16365.7402\n",
      "Epoch 74/1000\n",
      "80/80 [==============================] - 0s 14us/sample - loss: 21562.5736 - val_loss: 16350.4590\n",
      "Epoch 75/1000\n",
      "80/80 [==============================] - 0s 18us/sample - loss: 21534.2505 - val_loss: 16335.2012\n",
      "Epoch 76/1000\n",
      "80/80 [==============================] - 0s 27us/sample - loss: 21509.9763 - val_loss: 16319.2637\n",
      "Epoch 77/1000\n",
      "80/80 [==============================] - 0s 118us/sample - loss: 21478.6111 - val_loss: 16303.4199\n",
      "Epoch 78/1000\n",
      "80/80 [==============================] - 0s 41us/sample - loss: 21451.4304 - val_loss: 16287.1221\n",
      "Epoch 79/1000\n",
      "80/80 [==============================] - 0s 28us/sample - loss: 21421.9170 - val_loss: 16270.5283\n",
      "Epoch 80/1000\n",
      "80/80 [==============================] - 0s 30us/sample - loss: 21392.1458 - val_loss: 16253.1465\n",
      "Epoch 81/1000\n",
      "80/80 [==============================] - 0s 25us/sample - loss: 21361.4590 - val_loss: 16235.5645\n",
      "Epoch 82/1000\n",
      "80/80 [==============================] - 0s 21us/sample - loss: 21332.2620 - val_loss: 16218.2500\n",
      "Epoch 83/1000\n",
      "80/80 [==============================] - 0s 21us/sample - loss: 21296.1157 - val_loss: 16200.9971\n",
      "Epoch 84/1000\n",
      "80/80 [==============================] - 0s 16us/sample - loss: 21262.6968 - val_loss: 16183.3672\n",
      "Epoch 85/1000\n",
      "80/80 [==============================] - 0s 16us/sample - loss: 21228.3604 - val_loss: 16164.9629\n",
      "Epoch 86/1000\n",
      "80/80 [==============================] - 0s 21us/sample - loss: 21191.0894 - val_loss: 16145.6230\n",
      "Epoch 87/1000\n",
      "80/80 [==============================] - 0s 30us/sample - loss: 21159.2070 - val_loss: 16125.1270\n",
      "Epoch 88/1000\n",
      "80/80 [==============================] - 0s 39us/sample - loss: 21119.4805 - val_loss: 16104.0498\n",
      "Epoch 89/1000\n",
      "80/80 [==============================] - 0s 67us/sample - loss: 21081.9937 - val_loss: 16082.3750\n",
      "Epoch 90/1000\n",
      "80/80 [==============================] - 0s 30us/sample - loss: 21040.2095 - val_loss: 16059.8438\n",
      "Epoch 91/1000\n",
      "80/80 [==============================] - 0s 21us/sample - loss: 20999.4900 - val_loss: 16036.8877\n",
      "Epoch 92/1000\n",
      "80/80 [==============================] - 0s 22us/sample - loss: 20959.8154 - val_loss: 16013.0889\n",
      "Epoch 93/1000\n",
      "80/80 [==============================] - 0s 18us/sample - loss: 20915.5757 - val_loss: 15989.0811\n",
      "Epoch 94/1000\n",
      "80/80 [==============================] - 0s 26us/sample - loss: 20870.8582 - val_loss: 15963.9941\n",
      "Epoch 95/1000\n",
      "80/80 [==============================] - 0s 16us/sample - loss: 20825.7766 - val_loss: 15938.2656\n",
      "Epoch 96/1000\n",
      "80/80 [==============================] - 0s 25us/sample - loss: 20782.8264 - val_loss: 15912.3252\n",
      "Epoch 97/1000\n",
      "80/80 [==============================] - 0s 40us/sample - loss: 20732.6577 - val_loss: 15886.2090\n",
      "Epoch 98/1000\n",
      "80/80 [==============================] - 0s 21us/sample - loss: 20681.4221 - val_loss: 15859.7676\n",
      "Epoch 99/1000\n",
      "80/80 [==============================] - 0s 23us/sample - loss: 20633.5935 - val_loss: 15832.1035\n",
      "Epoch 100/1000\n",
      "80/80 [==============================] - 0s 24us/sample - loss: 20577.7854 - val_loss: 15803.8418\n",
      "Epoch 101/1000\n",
      "80/80 [==============================] - 0s 20us/sample - loss: 20527.7897 - val_loss: 15774.6719\n",
      "Epoch 102/1000\n",
      "80/80 [==============================] - 0s 27us/sample - loss: 20476.2454 - val_loss: 15745.1826\n",
      "Epoch 103/1000\n",
      "80/80 [==============================] - 0s 22us/sample - loss: 20415.3921 - val_loss: 15715.9893\n",
      "Epoch 104/1000\n",
      "80/80 [==============================] - 0s 23us/sample - loss: 20363.6072 - val_loss: 15685.5811\n",
      "Epoch 105/1000\n",
      "80/80 [==============================] - 0s 39us/sample - loss: 20302.3760 - val_loss: 15655.1660\n",
      "Epoch 106/1000\n",
      "80/80 [==============================] - 0s 171us/sample - loss: 20241.9403 - val_loss: 15624.2080\n",
      "Epoch 107/1000\n",
      "80/80 [==============================] - 0s 101us/sample - loss: 20179.6149 - val_loss: 15593.0078\n",
      "Epoch 108/1000\n",
      "80/80 [==============================] - 0s 22us/sample - loss: 20122.4944 - val_loss: 15560.0596\n",
      "Epoch 109/1000\n",
      "80/80 [==============================] - 0s 51us/sample - loss: 20059.3447 - val_loss: 15526.6533\n",
      "Epoch 110/1000\n",
      "80/80 [==============================] - 0s 36us/sample - loss: 19999.2773 - val_loss: 15492.7363\n",
      "Epoch 111/1000\n",
      "80/80 [==============================] - 0s 24us/sample - loss: 19929.2118 - val_loss: 15459.0762\n",
      "Epoch 112/1000\n",
      "80/80 [==============================] - 0s 20us/sample - loss: 19861.6863 - val_loss: 15425.0020\n",
      "Epoch 113/1000\n",
      "80/80 [==============================] - 0s 21us/sample - loss: 19797.1855 - val_loss: 15390.2783\n",
      "Epoch 114/1000\n",
      "80/80 [==============================] - 0s 27us/sample - loss: 19724.9863 - val_loss: 15356.2314\n",
      "Epoch 115/1000\n",
      "80/80 [==============================] - 0s 19us/sample - loss: 19661.1191 - val_loss: 15320.6562\n",
      "Epoch 116/1000\n",
      "80/80 [==============================] - 0s 16us/sample - loss: 19590.6167 - val_loss: 15284.1816\n",
      "Epoch 117/1000\n",
      "80/80 [==============================] - 0s 22us/sample - loss: 19512.0933 - val_loss: 15247.9785\n",
      "Epoch 118/1000\n",
      "80/80 [==============================] - 0s 17us/sample - loss: 19443.3861 - val_loss: 15209.7998\n",
      "Epoch 119/1000\n",
      "80/80 [==============================] - 0s 18us/sample - loss: 19372.2382 - val_loss: 15170.6191\n",
      "Epoch 120/1000\n",
      "80/80 [==============================] - 0s 19us/sample - loss: 19293.9521 - val_loss: 15132.1094\n",
      "Epoch 121/1000\n",
      "80/80 [==============================] - 0s 18us/sample - loss: 19214.5197 - val_loss: 15093.4404\n",
      "Epoch 122/1000\n",
      "80/80 [==============================] - 0s 16us/sample - loss: 19144.5554 - val_loss: 15053.5840\n",
      "Epoch 123/1000\n",
      "80/80 [==============================] - 0s 20us/sample - loss: 19063.7859 - val_loss: 15014.0566\n",
      "Epoch 124/1000\n",
      "80/80 [==============================] - 0s 20us/sample - loss: 18985.3083 - val_loss: 14974.3701\n",
      "Epoch 125/1000\n",
      "80/80 [==============================] - 0s 19us/sample - loss: 18901.5444 - val_loss: 14934.8906\n",
      "Epoch 126/1000\n",
      "80/80 [==============================] - 0s 16us/sample - loss: 18823.8949 - val_loss: 14893.7109\n",
      "Epoch 127/1000\n",
      "80/80 [==============================] - 0s 16us/sample - loss: 18741.7141 - val_loss: 14852.4453\n",
      "Epoch 128/1000\n",
      "80/80 [==============================] - 0s 69us/sample - loss: 18660.7900 - val_loss: 14810.7988\n",
      "Epoch 129/1000\n",
      "80/80 [==============================] - 0s 197us/sample - loss: 18573.4863 - val_loss: 14769.3887\n",
      "Epoch 130/1000\n",
      "80/80 [==============================] - 0s 25us/sample - loss: 18494.5317 - val_loss: 14727.4922\n",
      "Epoch 131/1000\n",
      "80/80 [==============================] - 0s 72us/sample - loss: 18397.7131 - val_loss: 14685.6309\n",
      "Epoch 132/1000\n",
      "80/80 [==============================] - 0s 26us/sample - loss: 18314.1522 - val_loss: 14642.8936\n",
      "Epoch 133/1000\n",
      "80/80 [==============================] - 0s 38us/sample - loss: 18221.2119 - val_loss: 14599.7471\n",
      "Epoch 134/1000\n",
      "80/80 [==============================] - 0s 24us/sample - loss: 18139.2769 - val_loss: 14555.5684\n",
      "Epoch 135/1000\n",
      "80/80 [==============================] - 0s 31us/sample - loss: 18037.1179 - val_loss: 14511.4941\n",
      "Epoch 136/1000\n",
      "80/80 [==============================] - 0s 21us/sample - loss: 17949.7690 - val_loss: 14466.7910\n",
      "Epoch 137/1000\n",
      "80/80 [==============================] - 0s 30us/sample - loss: 17858.6111 - val_loss: 14421.1465\n",
      "Epoch 138/1000\n",
      "80/80 [==============================] - 0s 32us/sample - loss: 17757.6392 - val_loss: 14375.1816\n",
      "Epoch 139/1000\n",
      "80/80 [==============================] - 0s 25us/sample - loss: 17658.5530 - val_loss: 14328.7861\n",
      "Epoch 140/1000\n",
      "80/80 [==============================] - 0s 18us/sample - loss: 17576.8008 - val_loss: 14282.1533\n",
      "Epoch 141/1000\n",
      "80/80 [==============================] - 0s 24us/sample - loss: 17474.3817 - val_loss: 14235.5967\n",
      "Epoch 142/1000\n",
      "80/80 [==============================] - 0s 38us/sample - loss: 17376.5571 - val_loss: 14189.1973\n",
      "Epoch 143/1000\n",
      "80/80 [==============================] - 0s 36us/sample - loss: 17274.5344 - val_loss: 14143.9629\n",
      "Epoch 144/1000\n",
      "80/80 [==============================] - 0s 29us/sample - loss: 17187.7015 - val_loss: 14098.0781\n",
      "Epoch 145/1000\n",
      "80/80 [==============================] - 0s 42us/sample - loss: 17083.5133 - val_loss: 14053.2998\n",
      "Epoch 146/1000\n",
      "80/80 [==============================] - 0s 37us/sample - loss: 16988.8912 - val_loss: 14008.8779\n",
      "Epoch 147/1000\n",
      "80/80 [==============================] - 0s 171us/sample - loss: 16889.4689 - val_loss: 13965.0029\n",
      "Epoch 148/1000\n",
      "80/80 [==============================] - 0s 143us/sample - loss: 16792.6509 - val_loss: 13920.9160\n",
      "Epoch 149/1000\n",
      "80/80 [==============================] - 0s 23us/sample - loss: 16696.8204 - val_loss: 13876.9375\n",
      "Epoch 150/1000\n",
      "80/80 [==============================] - 0s 83us/sample - loss: 16600.8689 - val_loss: 13833.9160\n",
      "Epoch 151/1000\n",
      "80/80 [==============================] - 0s 27us/sample - loss: 16501.7451 - val_loss: 13791.3955\n",
      "Epoch 152/1000\n",
      "80/80 [==============================] - 0s 30us/sample - loss: 16405.5249 - val_loss: 13748.4238\n",
      "Epoch 153/1000\n",
      "80/80 [==============================] - 0s 23us/sample - loss: 16300.0403 - val_loss: 13706.2627\n",
      "Epoch 154/1000\n",
      "80/80 [==============================] - 0s 25us/sample - loss: 16209.1567 - val_loss: 13663.4219\n",
      "Epoch 155/1000\n",
      "80/80 [==============================] - 0s 23us/sample - loss: 16108.2705 - val_loss: 13619.9014\n",
      "Epoch 156/1000\n",
      "80/80 [==============================] - 0s 21us/sample - loss: 16017.0869 - val_loss: 13575.7705\n",
      "Epoch 157/1000\n",
      "80/80 [==============================] - 0s 19us/sample - loss: 15905.5896 - val_loss: 13532.9199\n",
      "Epoch 158/1000\n",
      "80/80 [==============================] - 0s 20us/sample - loss: 15809.1832 - val_loss: 13489.8096\n",
      "Epoch 159/1000\n",
      "80/80 [==============================] - 0s 23us/sample - loss: 15714.8578 - val_loss: 13445.1816\n",
      "Epoch 160/1000\n",
      "80/80 [==============================] - 0s 22us/sample - loss: 15614.7937 - val_loss: 13400.7705\n",
      "Epoch 161/1000\n",
      "80/80 [==============================] - 0s 21us/sample - loss: 15517.5189 - val_loss: 13356.7734\n",
      "Epoch 162/1000\n",
      "80/80 [==============================] - 0s 23us/sample - loss: 15408.1086 - val_loss: 13314.1309\n",
      "Epoch 163/1000\n",
      "80/80 [==============================] - 0s 18us/sample - loss: 15317.0282 - val_loss: 13271.0107\n",
      "Epoch 164/1000\n",
      "80/80 [==============================] - 0s 21us/sample - loss: 15221.0378 - val_loss: 13227.9951\n",
      "Epoch 165/1000\n",
      "80/80 [==============================] - 0s 23us/sample - loss: 15121.6763 - val_loss: 13185.9121\n",
      "Epoch 166/1000\n",
      "80/80 [==============================] - 0s 17us/sample - loss: 15021.4269 - val_loss: 13144.1797\n",
      "Epoch 167/1000\n",
      "80/80 [==============================] - 0s 19us/sample - loss: 14914.3909 - val_loss: 13103.6318\n",
      "Epoch 168/1000\n",
      "80/80 [==============================] - 0s 18us/sample - loss: 14815.5800 - val_loss: 13063.7471\n",
      "Epoch 169/1000\n",
      "80/80 [==============================] - 0s 21us/sample - loss: 14718.8278 - val_loss: 13023.4893\n",
      "Epoch 170/1000\n",
      "80/80 [==============================] - 0s 64us/sample - loss: 14620.1750 - val_loss: 12984.8096\n",
      "Epoch 171/1000\n",
      "80/80 [==============================] - 0s 186us/sample - loss: 14511.8789 - val_loss: 12946.3789\n",
      "Epoch 172/1000\n",
      "80/80 [==============================] - 0s 32us/sample - loss: 14415.0797 - val_loss: 12907.4424\n",
      "Epoch 173/1000\n",
      "80/80 [==============================] - 0s 44us/sample - loss: 14319.8962 - val_loss: 12868.9785\n",
      "Epoch 174/1000\n",
      "80/80 [==============================] - 0s 29us/sample - loss: 14215.4714 - val_loss: 12831.3984\n",
      "Epoch 175/1000\n",
      "80/80 [==============================] - 0s 20us/sample - loss: 14105.7415 - val_loss: 12795.1172\n",
      "Epoch 176/1000\n",
      "80/80 [==============================] - 0s 22us/sample - loss: 14005.3536 - val_loss: 12758.9629\n",
      "Epoch 177/1000\n",
      "80/80 [==============================] - 0s 21us/sample - loss: 13901.3635 - val_loss: 12722.7637\n",
      "Epoch 178/1000\n",
      "80/80 [==============================] - 0s 17us/sample - loss: 13804.9150 - val_loss: 12687.4121\n",
      "Epoch 179/1000\n",
      "80/80 [==============================] - 0s 19us/sample - loss: 13695.9597 - val_loss: 12652.2441\n",
      "Epoch 180/1000\n",
      "80/80 [==============================] - 0s 19us/sample - loss: 13590.4047 - val_loss: 12618.6836\n",
      "Epoch 181/1000\n",
      "80/80 [==============================] - 0s 19us/sample - loss: 13489.4672 - val_loss: 12586.7871\n",
      "Epoch 182/1000\n",
      "80/80 [==============================] - 0s 18us/sample - loss: 13382.5601 - val_loss: 12554.7041\n",
      "Epoch 183/1000\n",
      "80/80 [==============================] - 0s 19us/sample - loss: 13274.2072 - val_loss: 12523.4219\n",
      "Epoch 184/1000\n",
      "80/80 [==============================] - 0s 18us/sample - loss: 13176.0356 - val_loss: 12491.8311\n",
      "Epoch 185/1000\n",
      "80/80 [==============================] - 0s 15us/sample - loss: 13064.8673 - val_loss: 12460.6113\n",
      "Epoch 186/1000\n",
      "80/80 [==============================] - 0s 17us/sample - loss: 12957.2029 - val_loss: 12430.7080\n",
      "Epoch 187/1000\n",
      "80/80 [==============================] - 0s 17us/sample - loss: 12857.4388 - val_loss: 12400.7568\n",
      "Epoch 188/1000\n",
      "80/80 [==============================] - 0s 21us/sample - loss: 12744.4645 - val_loss: 12370.5781\n",
      "Epoch 189/1000\n",
      "80/80 [==============================] - 0s 17us/sample - loss: 12638.4146 - val_loss: 12340.9434\n",
      "Epoch 190/1000\n",
      "80/80 [==============================] - 0s 18us/sample - loss: 12528.6556 - val_loss: 12311.8105\n",
      "Epoch 191/1000\n",
      "80/80 [==============================] - 0s 17us/sample - loss: 12415.5149 - val_loss: 12284.7432\n",
      "Epoch 192/1000\n",
      "80/80 [==============================] - 0s 17us/sample - loss: 12312.9323 - val_loss: 12257.5576\n",
      "Epoch 193/1000\n",
      "80/80 [==============================] - 0s 15us/sample - loss: 12206.7060 - val_loss: 12230.8623\n",
      "Epoch 194/1000\n",
      "80/80 [==============================] - 0s 15us/sample - loss: 12093.0970 - val_loss: 12204.9590\n",
      "Epoch 195/1000\n",
      "80/80 [==============================] - 0s 91us/sample - loss: 11968.1815 - val_loss: 12179.1846\n",
      "Epoch 196/1000\n",
      "80/80 [==============================] - 0s 162us/sample - loss: 11858.3717 - val_loss: 12154.0020\n",
      "Epoch 197/1000\n",
      "80/80 [==============================] - 0s 21us/sample - loss: 11756.4144 - val_loss: 12129.6230\n",
      "Epoch 198/1000\n",
      "80/80 [==============================] - 0s 17us/sample - loss: 11633.9353 - val_loss: 12105.9287\n",
      "Epoch 199/1000\n",
      "80/80 [==============================] - 0s 19us/sample - loss: 11514.5297 - val_loss: 12082.0186\n",
      "Epoch 200/1000\n",
      "80/80 [==============================] - 0s 27us/sample - loss: 11402.1310 - val_loss: 12059.0488\n",
      "Epoch 201/1000\n",
      "80/80 [==============================] - 0s 17us/sample - loss: 11290.7680 - val_loss: 12036.3770\n",
      "Epoch 202/1000\n",
      "80/80 [==============================] - 0s 17us/sample - loss: 11162.2493 - val_loss: 12014.5410\n",
      "Epoch 203/1000\n",
      "80/80 [==============================] - 0s 61us/sample - loss: 11047.6835 - val_loss: 11992.4365\n",
      "Epoch 204/1000\n",
      "80/80 [==============================] - 0s 24us/sample - loss: 10917.8779 - val_loss: 11969.4434\n",
      "Epoch 205/1000\n",
      "80/80 [==============================] - 0s 22us/sample - loss: 10796.2834 - val_loss: 11945.5254\n",
      "Epoch 206/1000\n",
      "80/80 [==============================] - 0s 24us/sample - loss: 10672.8315 - val_loss: 11921.0889\n",
      "Epoch 207/1000\n",
      "80/80 [==============================] - 0s 20us/sample - loss: 10550.8033 - val_loss: 11898.0205\n",
      "Epoch 208/1000\n",
      "80/80 [==============================] - 0s 25us/sample - loss: 10424.9176 - val_loss: 11874.4131\n",
      "Epoch 209/1000\n",
      "80/80 [==============================] - 0s 17us/sample - loss: 10285.4619 - val_loss: 11852.1211\n",
      "Epoch 210/1000\n",
      "80/80 [==============================] - 0s 16us/sample - loss: 10162.6993 - val_loss: 11829.7227\n",
      "Epoch 211/1000\n",
      "80/80 [==============================] - 0s 15us/sample - loss: 10036.9598 - val_loss: 11807.3965\n",
      "Epoch 212/1000\n",
      "80/80 [==============================] - 0s 21us/sample - loss: 9897.8347 - val_loss: 11784.9844\n",
      "Epoch 213/1000\n",
      "80/80 [==============================] - 0s 16us/sample - loss: 9763.9120 - val_loss: 11762.4434\n",
      "Epoch 214/1000\n",
      "80/80 [==============================] - 0s 17us/sample - loss: 9638.4080 - val_loss: 11740.4316\n",
      "Epoch 215/1000\n",
      "80/80 [==============================] - 0s 16us/sample - loss: 9495.8416 - val_loss: 11718.6934\n",
      "Epoch 216/1000\n",
      "80/80 [==============================] - 0s 22us/sample - loss: 9349.5412 - val_loss: 11696.5400\n",
      "Epoch 217/1000\n",
      "80/80 [==============================] - 0s 16us/sample - loss: 9221.0330 - val_loss: 11674.4092\n",
      "Epoch 218/1000\n",
      "80/80 [==============================] - 0s 17us/sample - loss: 9073.2053 - val_loss: 11652.2930\n",
      "Epoch 219/1000\n",
      "80/80 [==============================] - 0s 23us/sample - loss: 8949.3190 - val_loss: 11629.1533\n",
      "Epoch 220/1000\n",
      "80/80 [==============================] - 0s 17us/sample - loss: 8810.4705 - val_loss: 11605.6465\n",
      "Epoch 221/1000\n",
      "80/80 [==============================] - 0s 110us/sample - loss: 8669.1843 - val_loss: 11583.5596\n",
      "Epoch 222/1000\n",
      "80/80 [==============================] - 0s 131us/sample - loss: 8521.7215 - val_loss: 11560.6172\n",
      "Epoch 223/1000\n",
      "80/80 [==============================] - 0s 18us/sample - loss: 8373.4206 - val_loss: 11538.6387\n",
      "Epoch 224/1000\n",
      "80/80 [==============================] - 0s 17us/sample - loss: 8234.4106 - val_loss: 11515.9609\n",
      "Epoch 225/1000\n",
      "80/80 [==============================] - 0s 38us/sample - loss: 8091.4026 - val_loss: 11494.4482\n",
      "Epoch 226/1000\n",
      "80/80 [==============================] - 0s 40us/sample - loss: 7945.7756 - val_loss: 11473.1592\n",
      "Epoch 227/1000\n",
      "80/80 [==============================] - 0s 19us/sample - loss: 7817.7567 - val_loss: 11452.8223\n",
      "Epoch 228/1000\n",
      "80/80 [==============================] - 0s 19us/sample - loss: 7638.3890 - val_loss: 11434.4160\n",
      "Epoch 229/1000\n",
      "80/80 [==============================] - 0s 20us/sample - loss: 7516.1576 - val_loss: 11415.8066\n",
      "Epoch 230/1000\n",
      "80/80 [==============================] - 0s 26us/sample - loss: 7361.2122 - val_loss: 11397.4434\n",
      "Epoch 231/1000\n",
      "80/80 [==============================] - 0s 26us/sample - loss: 7221.4495 - val_loss: 11378.8359\n",
      "Epoch 232/1000\n",
      "80/80 [==============================] - 0s 22us/sample - loss: 7067.5684 - val_loss: 11361.7705\n",
      "Epoch 233/1000\n",
      "80/80 [==============================] - 0s 18us/sample - loss: 6921.0157 - val_loss: 11345.9707\n",
      "Epoch 234/1000\n",
      "80/80 [==============================] - 0s 17us/sample - loss: 6770.1279 - val_loss: 11330.0879\n",
      "Epoch 235/1000\n",
      "80/80 [==============================] - 0s 16us/sample - loss: 6623.0853 - val_loss: 11313.1123\n",
      "Epoch 236/1000\n",
      "80/80 [==============================] - 0s 18us/sample - loss: 6468.4863 - val_loss: 11296.7832\n",
      "Epoch 237/1000\n",
      "80/80 [==============================] - 0s 20us/sample - loss: 6313.1644 - val_loss: 11280.0596\n",
      "Epoch 238/1000\n",
      "80/80 [==============================] - 0s 14us/sample - loss: 6165.4970 - val_loss: 11263.1826\n",
      "Epoch 239/1000\n",
      "80/80 [==============================] - 0s 13us/sample - loss: 6010.7227 - val_loss: 11246.8496\n",
      "Epoch 240/1000\n",
      "80/80 [==============================] - 0s 15us/sample - loss: 5886.6724 - val_loss: 11231.3838\n",
      "Epoch 241/1000\n",
      "80/80 [==============================] - 0s 15us/sample - loss: 5722.1599 - val_loss: 11217.9785\n",
      "Epoch 242/1000\n",
      "80/80 [==============================] - 0s 14us/sample - loss: 5567.0796 - val_loss: 11205.2402\n",
      "Epoch 243/1000\n",
      "80/80 [==============================] - 0s 16us/sample - loss: 5437.1962 - val_loss: 11194.3359\n",
      "Epoch 244/1000\n",
      "80/80 [==============================] - 0s 21us/sample - loss: 5268.0552 - val_loss: 11184.5752\n",
      "Epoch 245/1000\n",
      "80/80 [==============================] - 0s 19us/sample - loss: 5132.4518 - val_loss: 11175.0576\n",
      "Epoch 246/1000\n",
      "80/80 [==============================] - 0s 17us/sample - loss: 4986.5811 - val_loss: 11165.5498\n",
      "Epoch 247/1000\n",
      "80/80 [==============================] - 0s 17us/sample - loss: 4846.7534 - val_loss: 11156.6094\n",
      "Epoch 248/1000\n",
      "80/80 [==============================] - 0s 18us/sample - loss: 4704.5343 - val_loss: 11148.9014\n",
      "Epoch 249/1000\n",
      "80/80 [==============================] - 0s 16us/sample - loss: 4556.3174 - val_loss: 11142.1660\n",
      "Epoch 250/1000\n",
      "80/80 [==============================] - 0s 19us/sample - loss: 4428.4399 - val_loss: 11135.8330\n",
      "Epoch 251/1000\n",
      "80/80 [==============================] - 0s 17us/sample - loss: 4293.7250 - val_loss: 11130.9980\n",
      "Epoch 252/1000\n",
      "80/80 [==============================] - 0s 16us/sample - loss: 4168.3623 - val_loss: 11127.9912\n",
      "Epoch 253/1000\n",
      "80/80 [==============================] - 0s 40us/sample - loss: 4035.9823 - val_loss: 11125.8662\n",
      "Epoch 254/1000\n",
      "80/80 [==============================] - 0s 276us/sample - loss: 3896.3952 - val_loss: 11124.6152\n",
      "Epoch 255/1000\n",
      "80/80 [==============================] - 0s 26us/sample - loss: 3761.3387 - val_loss: 11124.2949\n",
      "Epoch 256/1000\n",
      "80/80 [==============================] - 0s 24us/sample - loss: 3645.1442 - val_loss: 11125.7266\n",
      "Epoch 257/1000\n",
      "80/80 [==============================] - 0s 52us/sample - loss: 3524.8878 - val_loss: 11128.2451\n",
      "Epoch 258/1000\n",
      "80/80 [==============================] - 0s 37us/sample - loss: 3410.7722 - val_loss: 11131.0840\n",
      "Epoch 259/1000\n",
      "80/80 [==============================] - 0s 26us/sample - loss: 3297.9481 - val_loss: 11133.3564\n",
      "Epoch 260/1000\n",
      "80/80 [==============================] - 0s 25us/sample - loss: 3182.6309 - val_loss: 11137.4072\n",
      "Epoch 261/1000\n",
      "80/80 [==============================] - 0s 25us/sample - loss: 3068.2138 - val_loss: 11140.8926\n",
      "Epoch 262/1000\n",
      "80/80 [==============================] - 0s 22us/sample - loss: 2961.5156 - val_loss: 11144.6084\n",
      "Epoch 263/1000\n",
      "80/80 [==============================] - 0s 23us/sample - loss: 2857.8145 - val_loss: 11148.6748\n",
      "Epoch 264/1000\n",
      "80/80 [==============================] - 0s 20us/sample - loss: 2763.7607 - val_loss: 11152.8262\n",
      "Epoch 265/1000\n",
      "80/80 [==============================] - 0s 24us/sample - loss: 2659.5126 - val_loss: 11157.5596\n",
      "Epoch 266/1000\n",
      "80/80 [==============================] - 0s 28us/sample - loss: 2562.2789 - val_loss: 11162.2764\n",
      "Epoch 267/1000\n",
      "80/80 [==============================] - 0s 18us/sample - loss: 2478.6580 - val_loss: 11168.3076\n",
      "Epoch 268/1000\n",
      "80/80 [==============================] - 0s 19us/sample - loss: 2390.7131 - val_loss: 11175.6953\n",
      "Epoch 269/1000\n",
      "80/80 [==============================] - 0s 18us/sample - loss: 2316.5138 - val_loss: 11183.3389\n",
      "Epoch 270/1000\n",
      "80/80 [==============================] - 0s 18us/sample - loss: 2226.9121 - val_loss: 11191.5830\n",
      "Epoch 271/1000\n",
      "80/80 [==============================] - 0s 18us/sample - loss: 2152.1818 - val_loss: 11199.1973\n",
      "Epoch 272/1000\n",
      "80/80 [==============================] - 0s 19us/sample - loss: 2075.3374 - val_loss: 11208.5811\n",
      "Epoch 273/1000\n",
      "80/80 [==============================] - 0s 18us/sample - loss: 2003.7809 - val_loss: 11217.7080\n",
      "Epoch 274/1000\n",
      "80/80 [==============================] - 0s 18us/sample - loss: 1945.7658 - val_loss: 11227.8076\n",
      "Epoch 275/1000\n",
      "80/80 [==============================] - 0s 25us/sample - loss: 1867.3444 - val_loss: 11237.3232\n",
      "Epoch 276/1000\n",
      "80/80 [==============================] - 0s 25us/sample - loss: 1812.4599 - val_loss: 11247.5879\n",
      "Epoch 277/1000\n",
      "80/80 [==============================] - 0s 27us/sample - loss: 1760.4312 - val_loss: 11258.6670\n",
      "Epoch 278/1000\n",
      "80/80 [==============================] - 0s 23us/sample - loss: 1695.4613 - val_loss: 11269.2402\n",
      "Epoch 279/1000\n",
      "80/80 [==============================] - 0s 17us/sample - loss: 1637.3389 - val_loss: 11279.7871\n",
      "Epoch 280/1000\n",
      "80/80 [==============================] - 0s 16us/sample - loss: 1594.1102 - val_loss: 11290.2314\n",
      "Epoch 281/1000\n",
      "80/80 [==============================] - 0s 17us/sample - loss: 1541.2594 - val_loss: 11301.7871\n",
      "Epoch 282/1000\n",
      "80/80 [==============================] - 0s 20us/sample - loss: 1497.2834 - val_loss: 11313.3154\n",
      "Epoch 283/1000\n",
      "80/80 [==============================] - 0s 22us/sample - loss: 1446.5774 - val_loss: 11324.3496\n",
      "Epoch 284/1000\n",
      "80/80 [==============================] - 0s 273us/sample - loss: 1407.4226 - val_loss: 11335.9316\n",
      "Epoch 285/1000\n",
      "80/80 [==============================] - 0s 20us/sample - loss: 1367.2840 - val_loss: 11347.4326\n",
      "Epoch 286/1000\n",
      "80/80 [==============================] - 0s 17us/sample - loss: 1325.2695 - val_loss: 11357.5889\n",
      "Epoch 287/1000\n",
      "80/80 [==============================] - 0s 23us/sample - loss: 1289.7050 - val_loss: 11368.3438\n",
      "Epoch 288/1000\n",
      "80/80 [==============================] - 0s 19us/sample - loss: 1252.4135 - val_loss: 11380.8467\n",
      "Epoch 289/1000\n",
      "80/80 [==============================] - 0s 78us/sample - loss: 1218.1258 - val_loss: 11394.3877\n",
      "Epoch 290/1000\n",
      "80/80 [==============================] - 0s 20us/sample - loss: 1186.4609 - val_loss: 11407.2061\n",
      "Epoch 291/1000\n",
      "80/80 [==============================] - 0s 18us/sample - loss: 1155.4333 - val_loss: 11420.2520\n",
      "Epoch 292/1000\n",
      "80/80 [==============================] - 0s 18us/sample - loss: 1125.8004 - val_loss: 11432.4580\n",
      "Epoch 293/1000\n",
      "80/80 [==============================] - 0s 25us/sample - loss: 1092.3264 - val_loss: 11443.7012\n",
      "Epoch 294/1000\n",
      "80/80 [==============================] - 0s 24us/sample - loss: 1068.8437 - val_loss: 11455.3389\n",
      "Epoch 295/1000\n",
      "80/80 [==============================] - 0s 21us/sample - loss: 1039.9501 - val_loss: 11466.4941\n",
      "Epoch 296/1000\n",
      "80/80 [==============================] - 0s 18us/sample - loss: 1016.7921 - val_loss: 11477.5332\n",
      "Epoch 297/1000\n",
      "80/80 [==============================] - 0s 17us/sample - loss: 991.2616 - val_loss: 11488.2109\n",
      "Epoch 298/1000\n",
      "80/80 [==============================] - 0s 18us/sample - loss: 968.2496 - val_loss: 11498.3369\n",
      "Epoch 299/1000\n",
      "80/80 [==============================] - 0s 16us/sample - loss: 943.1916 - val_loss: 11507.2109\n",
      "Epoch 300/1000\n",
      "80/80 [==============================] - 0s 16us/sample - loss: 921.7812 - val_loss: 11515.2812\n",
      "Epoch 301/1000\n",
      "80/80 [==============================] - 0s 22us/sample - loss: 902.3090 - val_loss: 11523.5654\n",
      "Epoch 302/1000\n",
      "80/80 [==============================] - 0s 20us/sample - loss: 880.9474 - val_loss: 11531.7627\n",
      "Epoch 303/1000\n",
      "80/80 [==============================] - 0s 18us/sample - loss: 862.9981 - val_loss: 11539.3574\n",
      "Epoch 304/1000\n",
      "80/80 [==============================] - 0s 16us/sample - loss: 842.5463 - val_loss: 11546.1006\n",
      "Epoch 305/1000\n",
      "80/80 [==============================] - 0s 14us/sample - loss: 823.2303 - val_loss: 11554.5361\n",
      "Epoch 306/1000\n",
      "80/80 [==============================] - 0s 18us/sample - loss: 802.7444 - val_loss: 11563.7129\n",
      "Epoch 307/1000\n",
      "80/80 [==============================] - 0s 20us/sample - loss: 787.0759 - val_loss: 11574.5654\n",
      "Epoch 308/1000\n",
      "80/80 [==============================] - 0s 17us/sample - loss: 767.2688 - val_loss: 11585.1846\n",
      "Epoch 309/1000\n",
      "80/80 [==============================] - 0s 16us/sample - loss: 750.9716 - val_loss: 11596.1670\n",
      "Epoch 310/1000\n",
      "80/80 [==============================] - 0s 16us/sample - loss: 733.6188 - val_loss: 11607.2490\n",
      "Epoch 311/1000\n",
      "80/80 [==============================] - 0s 17us/sample - loss: 716.6636 - val_loss: 11617.4941\n",
      "Epoch 312/1000\n",
      "80/80 [==============================] - 0s 16us/sample - loss: 699.7591 - val_loss: 11627.1982\n",
      "Epoch 313/1000\n",
      "80/80 [==============================] - 0s 29us/sample - loss: 685.4203 - val_loss: 11637.0186\n",
      "Epoch 314/1000\n",
      "80/80 [==============================] - 0s 25us/sample - loss: 669.2073 - val_loss: 11645.2500\n",
      "Epoch 315/1000\n",
      "80/80 [==============================] - 0s 120us/sample - loss: 654.3287 - val_loss: 11653.1670\n",
      "Epoch 316/1000\n",
      "80/80 [==============================] - 0s 51us/sample - loss: 640.7483 - val_loss: 11662.0996\n",
      "Epoch 317/1000\n",
      "80/80 [==============================] - 0s 32us/sample - loss: 624.9166 - val_loss: 11670.3311\n",
      "Epoch 318/1000\n",
      "80/80 [==============================] - 0s 21us/sample - loss: 612.7570 - val_loss: 11677.5615\n",
      "Epoch 319/1000\n",
      "80/80 [==============================] - 0s 17us/sample - loss: 599.5137 - val_loss: 11684.9326\n",
      "Epoch 320/1000\n",
      "80/80 [==============================] - 0s 17us/sample - loss: 585.9365 - val_loss: 11691.8828\n",
      "Epoch 321/1000\n",
      "80/80 [==============================] - 0s 20us/sample - loss: 575.3490 - val_loss: 11697.1006\n",
      "Epoch 322/1000\n",
      "80/80 [==============================] - 0s 19us/sample - loss: 561.7226 - val_loss: 11701.6318\n",
      "Epoch 323/1000\n",
      "80/80 [==============================] - 0s 23us/sample - loss: 550.8381 - val_loss: 11706.6768\n",
      "Epoch 324/1000\n",
      "80/80 [==============================] - 0s 19us/sample - loss: 538.9806 - val_loss: 11712.3730\n",
      "Epoch 325/1000\n",
      "80/80 [==============================] - 0s 18us/sample - loss: 528.2262 - val_loss: 11717.5352\n",
      "Epoch 326/1000\n",
      "80/80 [==============================] - 0s 18us/sample - loss: 518.7018 - val_loss: 11722.2871\n",
      "Epoch 327/1000\n",
      "80/80 [==============================] - 0s 24us/sample - loss: 507.0344 - val_loss: 11725.1777\n",
      "Epoch 328/1000\n",
      "80/80 [==============================] - 0s 21us/sample - loss: 497.4963 - val_loss: 11727.3682\n",
      "Epoch 329/1000\n",
      "80/80 [==============================] - 0s 21us/sample - loss: 486.9384 - val_loss: 11728.8311\n",
      "Epoch 330/1000\n",
      "80/80 [==============================] - 0s 48us/sample - loss: 477.5314 - val_loss: 11731.7812\n",
      "Epoch 331/1000\n",
      "80/80 [==============================] - 0s 42us/sample - loss: 468.7407 - val_loss: 11734.4893\n",
      "Epoch 332/1000\n",
      "80/80 [==============================] - 0s 17us/sample - loss: 459.4018 - val_loss: 11735.4971\n",
      "Epoch 333/1000\n",
      "80/80 [==============================] - 0s 20us/sample - loss: 449.6903 - val_loss: 11737.0439\n",
      "Epoch 334/1000\n",
      "80/80 [==============================] - 0s 17us/sample - loss: 442.1757 - val_loss: 11737.6533\n",
      "Epoch 335/1000\n",
      "80/80 [==============================] - 0s 20us/sample - loss: 433.8991 - val_loss: 11738.4023\n",
      "Epoch 336/1000\n",
      "80/80 [==============================] - 0s 19us/sample - loss: 425.5404 - val_loss: 11739.1758\n",
      "Epoch 337/1000\n",
      "80/80 [==============================] - 0s 19us/sample - loss: 418.0480 - val_loss: 11740.7139\n",
      "Epoch 338/1000\n",
      "80/80 [==============================] - 0s 22us/sample - loss: 409.3005 - val_loss: 11741.2568\n",
      "Epoch 339/1000\n",
      "80/80 [==============================] - 0s 25us/sample - loss: 402.2827 - val_loss: 11741.5957\n",
      "Epoch 340/1000\n",
      "80/80 [==============================] - 0s 25us/sample - loss: 395.2116 - val_loss: 11741.3711\n",
      "Epoch 341/1000\n",
      "80/80 [==============================] - 0s 20us/sample - loss: 388.4061 - val_loss: 11741.2705\n",
      "Epoch 342/1000\n",
      "80/80 [==============================] - 0s 29us/sample - loss: 381.4268 - val_loss: 11742.0879\n",
      "Epoch 343/1000\n",
      "80/80 [==============================] - 0s 24us/sample - loss: 374.7090 - val_loss: 11741.8369\n",
      "Epoch 344/1000\n",
      "80/80 [==============================] - 0s 24us/sample - loss: 367.6039 - val_loss: 11741.5674\n",
      "Epoch 345/1000\n",
      "80/80 [==============================] - 0s 28us/sample - loss: 361.5977 - val_loss: 11741.5830\n",
      "Epoch 346/1000\n",
      "80/80 [==============================] - 0s 28us/sample - loss: 355.6879 - val_loss: 11742.2246\n",
      "Epoch 347/1000\n",
      "80/80 [==============================] - 0s 19us/sample - loss: 349.5514 - val_loss: 11742.8438\n",
      "Epoch 348/1000\n",
      "80/80 [==============================] - 0s 22us/sample - loss: 343.4984 - val_loss: 11743.6768\n",
      "Epoch 349/1000\n",
      "80/80 [==============================] - 0s 28us/sample - loss: 337.8764 - val_loss: 11744.5645\n",
      "Epoch 350/1000\n",
      "80/80 [==============================] - 0s 18us/sample - loss: 332.2497 - val_loss: 11745.3008\n",
      "Epoch 351/1000\n",
      "80/80 [==============================] - 0s 16us/sample - loss: 327.5075 - val_loss: 11744.9199\n",
      "Epoch 352/1000\n",
      "80/80 [==============================] - 0s 17us/sample - loss: 321.7081 - val_loss: 11744.8174\n",
      "Epoch 353/1000\n",
      "80/80 [==============================] - 0s 16us/sample - loss: 316.7233 - val_loss: 11744.4600\n",
      "Epoch 354/1000\n",
      "80/80 [==============================] - 0s 14us/sample - loss: 311.4905 - val_loss: 11744.1055\n",
      "Epoch 355/1000\n",
      "80/80 [==============================] - 0s 16us/sample - loss: 306.5073 - val_loss: 11743.0361\n",
      "Epoch 356/1000\n",
      "80/80 [==============================] - 0s 16us/sample - loss: 301.1537 - val_loss: 11740.7012\n",
      "Epoch 357/1000\n",
      "80/80 [==============================] - 0s 19us/sample - loss: 297.2267 - val_loss: 11738.3457\n",
      "Epoch 358/1000\n",
      "80/80 [==============================] - 0s 16us/sample - loss: 292.6479 - val_loss: 11735.5957\n",
      "Epoch 359/1000\n",
      "80/80 [==============================] - 0s 18us/sample - loss: 287.6313 - val_loss: 11733.0557\n",
      "Epoch 360/1000\n",
      "80/80 [==============================] - 0s 19us/sample - loss: 282.9491 - val_loss: 11731.1670\n",
      "Epoch 361/1000\n",
      "80/80 [==============================] - 0s 19us/sample - loss: 279.1225 - val_loss: 11729.5459\n",
      "Epoch 362/1000\n",
      "80/80 [==============================] - 0s 21us/sample - loss: 274.7449 - val_loss: 11727.0352\n",
      "Epoch 363/1000\n",
      "80/80 [==============================] - 0s 19us/sample - loss: 270.1754 - val_loss: 11725.0742\n",
      "Epoch 364/1000\n",
      "80/80 [==============================] - 0s 29us/sample - loss: 266.4713 - val_loss: 11723.3086\n",
      "Epoch 365/1000\n",
      "80/80 [==============================] - 0s 23us/sample - loss: 262.6169 - val_loss: 11721.9307\n",
      "Epoch 366/1000\n",
      "80/80 [==============================] - 0s 19us/sample - loss: 258.3391 - val_loss: 11721.6846\n",
      "Epoch 367/1000\n",
      "80/80 [==============================] - 0s 32us/sample - loss: 254.8832 - val_loss: 11721.9404\n",
      "Epoch 368/1000\n",
      "80/80 [==============================] - 0s 21us/sample - loss: 251.1585 - val_loss: 11722.9482\n",
      "Epoch 369/1000\n",
      "80/80 [==============================] - 0s 25us/sample - loss: 247.7710 - val_loss: 11723.1191\n",
      "Epoch 370/1000\n",
      "80/80 [==============================] - 0s 21us/sample - loss: 244.6560 - val_loss: 11723.0049\n",
      "Epoch 371/1000\n",
      "80/80 [==============================] - 0s 17us/sample - loss: 240.9024 - val_loss: 11722.8184\n",
      "Epoch 372/1000\n",
      "80/80 [==============================] - 0s 16us/sample - loss: 237.6720 - val_loss: 11721.8877\n",
      "Epoch 373/1000\n",
      "80/80 [==============================] - 0s 20us/sample - loss: 234.4477 - val_loss: 11721.3564\n",
      "Epoch 374/1000\n",
      "80/80 [==============================] - 0s 20us/sample - loss: 231.4172 - val_loss: 11720.9629\n",
      "Epoch 375/1000\n",
      "80/80 [==============================] - 0s 21us/sample - loss: 228.1377 - val_loss: 11720.6133\n",
      "Epoch 376/1000\n",
      "80/80 [==============================] - 0s 42us/sample - loss: 225.6261 - val_loss: 11720.0254\n",
      "Epoch 377/1000\n",
      "80/80 [==============================] - 0s 205us/sample - loss: 222.5605 - val_loss: 11719.9297\n",
      "Epoch 378/1000\n",
      "80/80 [==============================] - 0s 31us/sample - loss: 219.9397 - val_loss: 11720.1387\n",
      "Epoch 379/1000\n",
      "80/80 [==============================] - 0s 25us/sample - loss: 216.9807 - val_loss: 11719.9082\n",
      "Epoch 380/1000\n",
      "80/80 [==============================] - 0s 22us/sample - loss: 213.9396 - val_loss: 11720.3721\n",
      "Epoch 381/1000\n",
      "80/80 [==============================] - 0s 108us/sample - loss: 211.4679 - val_loss: 11720.8779\n",
      "Epoch 382/1000\n",
      "80/80 [==============================] - 0s 27us/sample - loss: 209.3314 - val_loss: 11721.2803\n",
      "Epoch 383/1000\n",
      "80/80 [==============================] - 0s 24us/sample - loss: 206.5440 - val_loss: 11721.7168\n",
      "Epoch 384/1000\n",
      "80/80 [==============================] - 0s 19us/sample - loss: 203.9524 - val_loss: 11722.1113\n",
      "Epoch 385/1000\n",
      "80/80 [==============================] - 0s 30us/sample - loss: 201.6478 - val_loss: 11722.5850\n",
      "Epoch 386/1000\n",
      "80/80 [==============================] - 0s 18us/sample - loss: 199.1992 - val_loss: 11722.8359\n",
      "Epoch 387/1000\n",
      "80/80 [==============================] - 0s 16us/sample - loss: 197.1651 - val_loss: 11723.0684\n",
      "Epoch 388/1000\n",
      "80/80 [==============================] - 0s 19us/sample - loss: 194.7535 - val_loss: 11722.8379\n",
      "Epoch 389/1000\n",
      "80/80 [==============================] - 0s 15us/sample - loss: 192.4399 - val_loss: 11722.4561\n",
      "Epoch 390/1000\n",
      "80/80 [==============================] - 0s 17us/sample - loss: 190.5337 - val_loss: 11722.0605\n",
      "Epoch 391/1000\n",
      "80/80 [==============================] - 0s 22us/sample - loss: 188.4437 - val_loss: 11721.9551\n",
      "Epoch 392/1000\n",
      "80/80 [==============================] - 0s 24us/sample - loss: 186.2148 - val_loss: 11721.6768\n",
      "Epoch 393/1000\n",
      "80/80 [==============================] - 0s 18us/sample - loss: 184.3510 - val_loss: 11721.1729\n",
      "Epoch 394/1000\n",
      "80/80 [==============================] - 0s 17us/sample - loss: 182.5355 - val_loss: 11721.3906\n",
      "Epoch 395/1000\n",
      "80/80 [==============================] - 0s 17us/sample - loss: 180.4765 - val_loss: 11721.6367\n",
      "Epoch 396/1000\n",
      "80/80 [==============================] - 0s 17us/sample - loss: 178.6932 - val_loss: 11722.1953\n",
      "Epoch 397/1000\n",
      "80/80 [==============================] - 0s 15us/sample - loss: 176.8518 - val_loss: 11722.1982\n",
      "Epoch 398/1000\n",
      "80/80 [==============================] - 0s 23us/sample - loss: 175.0348 - val_loss: 11722.3398\n",
      "Epoch 399/1000\n",
      "80/80 [==============================] - 0s 18us/sample - loss: 173.4665 - val_loss: 11721.8857\n",
      "Epoch 400/1000\n",
      "80/80 [==============================] - 0s 27us/sample - loss: 171.5171 - val_loss: 11721.6797\n",
      "Epoch 401/1000\n",
      "80/80 [==============================] - 0s 56us/sample - loss: 170.1972 - val_loss: 11721.6162\n",
      "Epoch 402/1000\n",
      "80/80 [==============================] - 0s 342us/sample - loss: 168.2199 - val_loss: 11721.8516\n",
      "Epoch 403/1000\n",
      "80/80 [==============================] - 0s 26us/sample - loss: 166.6979 - val_loss: 11722.0430\n",
      "Epoch 404/1000\n",
      "80/80 [==============================] - 0s 42us/sample - loss: 165.0285 - val_loss: 11722.0518\n",
      "Epoch 405/1000\n",
      "80/80 [==============================] - 0s 67us/sample - loss: 163.5976 - val_loss: 11721.9580\n",
      "Epoch 406/1000\n",
      "80/80 [==============================] - 0s 34us/sample - loss: 162.1218 - val_loss: 11721.5449\n",
      "Epoch 407/1000\n",
      "80/80 [==============================] - 0s 22us/sample - loss: 160.6001 - val_loss: 11721.1494\n",
      "Epoch 408/1000\n",
      "80/80 [==============================] - 0s 16us/sample - loss: 159.0897 - val_loss: 11720.2686\n",
      "Epoch 409/1000\n",
      "80/80 [==============================] - 0s 20us/sample - loss: 157.5868 - val_loss: 11719.3369\n",
      "Epoch 410/1000\n",
      "80/80 [==============================] - 0s 16us/sample - loss: 156.1962 - val_loss: 11718.1113\n",
      "Epoch 411/1000\n",
      "80/80 [==============================] - 0s 17us/sample - loss: 154.6268 - val_loss: 11717.1074\n",
      "Epoch 412/1000\n",
      "80/80 [==============================] - 0s 15us/sample - loss: 153.2030 - val_loss: 11716.2109\n",
      "Epoch 413/1000\n",
      "80/80 [==============================] - 0s 16us/sample - loss: 151.9359 - val_loss: 11715.1562\n",
      "Epoch 414/1000\n",
      "80/80 [==============================] - 0s 21us/sample - loss: 150.5657 - val_loss: 11714.2754\n",
      "Epoch 415/1000\n",
      "80/80 [==============================] - 0s 19us/sample - loss: 149.2859 - val_loss: 11713.6396\n",
      "Epoch 416/1000\n",
      "80/80 [==============================] - 0s 21us/sample - loss: 148.1174 - val_loss: 11712.4990\n",
      "Epoch 417/1000\n",
      "80/80 [==============================] - 0s 25us/sample - loss: 146.7305 - val_loss: 11711.9248\n",
      "Epoch 418/1000\n",
      "80/80 [==============================] - 0s 18us/sample - loss: 145.6327 - val_loss: 11711.3809\n",
      "Epoch 419/1000\n",
      "80/80 [==============================] - 0s 18us/sample - loss: 144.5150 - val_loss: 11710.8096\n",
      "Epoch 420/1000\n",
      "80/80 [==============================] - 0s 29us/sample - loss: 143.3615 - val_loss: 11710.1768\n",
      "Epoch 421/1000\n",
      "80/80 [==============================] - 0s 21us/sample - loss: 142.2593 - val_loss: 11709.5625\n",
      "Epoch 422/1000\n",
      "80/80 [==============================] - 0s 16us/sample - loss: 141.2152 - val_loss: 11708.4219\n",
      "Epoch 423/1000\n",
      "80/80 [==============================] - 0s 25us/sample - loss: 140.0660 - val_loss: 11707.6064\n",
      "Epoch 424/1000\n",
      "80/80 [==============================] - 0s 26us/sample - loss: 139.1398 - val_loss: 11706.6846\n",
      "Epoch 425/1000\n",
      "80/80 [==============================] - 0s 19us/sample - loss: 138.0542 - val_loss: 11705.5332\n",
      "Epoch 426/1000\n",
      "80/80 [==============================] - 0s 15us/sample - loss: 137.1854 - val_loss: 11704.3076\n",
      "Epoch 427/1000\n",
      "80/80 [==============================] - 0s 18us/sample - loss: 136.2414 - val_loss: 11703.2734\n",
      "Epoch 428/1000\n",
      "80/80 [==============================] - 0s 21us/sample - loss: 135.2505 - val_loss: 11702.2861\n",
      "Epoch 429/1000\n",
      "80/80 [==============================] - 0s 15us/sample - loss: 134.3305 - val_loss: 11701.4941\n",
      "Epoch 430/1000\n",
      "80/80 [==============================] - 0s 15us/sample - loss: 133.4578 - val_loss: 11700.6035\n",
      "Epoch 431/1000\n",
      "80/80 [==============================] - 0s 16us/sample - loss: 132.6418 - val_loss: 11699.9268\n",
      "Epoch 432/1000\n",
      "80/80 [==============================] - 0s 16us/sample - loss: 131.6517 - val_loss: 11699.2998\n",
      "Epoch 433/1000\n",
      "80/80 [==============================] - 0s 16us/sample - loss: 130.9015 - val_loss: 11699.0469\n",
      "Epoch 434/1000\n",
      "80/80 [==============================] - 0s 17us/sample - loss: 130.0243 - val_loss: 11698.8262\n",
      "Epoch 435/1000\n",
      "80/80 [==============================] - 0s 17us/sample - loss: 129.2231 - val_loss: 11698.7998\n",
      "Epoch 436/1000\n",
      "80/80 [==============================] - 0s 20us/sample - loss: 128.4512 - val_loss: 11698.7852\n",
      "Epoch 437/1000\n",
      "80/80 [==============================] - 0s 17us/sample - loss: 127.6230 - val_loss: 11698.6992\n",
      "Epoch 438/1000\n",
      "80/80 [==============================] - 0s 18us/sample - loss: 126.9242 - val_loss: 11698.7432\n",
      "Epoch 439/1000\n",
      "80/80 [==============================] - 0s 19us/sample - loss: 126.1437 - val_loss: 11698.5078\n",
      "Epoch 440/1000\n",
      "80/80 [==============================] - 0s 18us/sample - loss: 125.4125 - val_loss: 11698.2012\n",
      "Epoch 441/1000\n",
      "80/80 [==============================] - 0s 15us/sample - loss: 124.6585 - val_loss: 11697.9062\n",
      "Epoch 442/1000\n",
      "80/80 [==============================] - 0s 14us/sample - loss: 123.9475 - val_loss: 11697.0781\n",
      "Epoch 443/1000\n",
      "80/80 [==============================] - 0s 19us/sample - loss: 123.2909 - val_loss: 11696.4033\n",
      "Epoch 444/1000\n",
      "80/80 [==============================] - 0s 16us/sample - loss: 122.5820 - val_loss: 11695.5576\n",
      "Epoch 445/1000\n",
      "80/80 [==============================] - 0s 17us/sample - loss: 121.8587 - val_loss: 11694.8281\n",
      "Epoch 446/1000\n",
      "80/80 [==============================] - 0s 21us/sample - loss: 121.2241 - val_loss: 11693.7764\n",
      "Epoch 447/1000\n",
      "80/80 [==============================] - 0s 22us/sample - loss: 120.5427 - val_loss: 11692.6641\n",
      "Epoch 448/1000\n",
      "80/80 [==============================] - 0s 196us/sample - loss: 119.9195 - val_loss: 11691.6846\n",
      "Epoch 449/1000\n",
      "80/80 [==============================] - 0s 62us/sample - loss: 119.3445 - val_loss: 11690.6230\n",
      "Epoch 450/1000\n",
      "80/80 [==============================] - 0s 22us/sample - loss: 118.6834 - val_loss: 11689.7832\n",
      "Epoch 451/1000\n",
      "80/80 [==============================] - 0s 24us/sample - loss: 117.9665 - val_loss: 11688.9473\n",
      "Epoch 452/1000\n",
      "80/80 [==============================] - 0s 18us/sample - loss: 117.4531 - val_loss: 11687.8936\n",
      "Epoch 453/1000\n",
      "80/80 [==============================] - 0s 22us/sample - loss: 116.8276 - val_loss: 11686.8428\n",
      "Epoch 454/1000\n",
      "80/80 [==============================] - 0s 21us/sample - loss: 116.2948 - val_loss: 11685.9482\n",
      "Epoch 455/1000\n",
      "80/80 [==============================] - 0s 20us/sample - loss: 115.6332 - val_loss: 11685.2744\n",
      "Epoch 456/1000\n",
      "80/80 [==============================] - 0s 19us/sample - loss: 115.1508 - val_loss: 11684.6123\n",
      "Epoch 457/1000\n",
      "80/80 [==============================] - 0s 18us/sample - loss: 114.5887 - val_loss: 11684.0000\n",
      "Epoch 458/1000\n",
      "80/80 [==============================] - 0s 58us/sample - loss: 114.0271 - val_loss: 11683.5098\n",
      "Epoch 459/1000\n",
      "80/80 [==============================] - 0s 43us/sample - loss: 113.5284 - val_loss: 11682.8164\n",
      "Epoch 460/1000\n",
      "80/80 [==============================] - 0s 24us/sample - loss: 113.0471 - val_loss: 11681.9434\n",
      "Epoch 461/1000\n",
      "80/80 [==============================] - 0s 22us/sample - loss: 112.5416 - val_loss: 11681.3447\n",
      "Epoch 462/1000\n",
      "80/80 [==============================] - 0s 22us/sample - loss: 112.0515 - val_loss: 11680.8301\n",
      "Epoch 463/1000\n",
      "80/80 [==============================] - 0s 23us/sample - loss: 111.5589 - val_loss: 11680.1143\n",
      "Epoch 464/1000\n",
      "80/80 [==============================] - 0s 19us/sample - loss: 111.1077 - val_loss: 11679.3867\n",
      "Epoch 465/1000\n",
      "80/80 [==============================] - 0s 17us/sample - loss: 110.5854 - val_loss: 11678.9111\n",
      "Epoch 466/1000\n",
      "80/80 [==============================] - 0s 19us/sample - loss: 110.1873 - val_loss: 11678.4668\n",
      "Epoch 467/1000\n",
      "80/80 [==============================] - 0s 16us/sample - loss: 109.7732 - val_loss: 11678.0576\n",
      "Epoch 468/1000\n",
      "80/80 [==============================] - 0s 17us/sample - loss: 109.3170 - val_loss: 11677.3340\n",
      "Epoch 469/1000\n",
      "80/80 [==============================] - 0s 24us/sample - loss: 108.8623 - val_loss: 11676.5381\n",
      "Epoch 470/1000\n",
      "80/80 [==============================] - 0s 17us/sample - loss: 108.4043 - val_loss: 11675.6543\n",
      "Epoch 471/1000\n",
      "80/80 [==============================] - 0s 19us/sample - loss: 107.9804 - val_loss: 11674.5625\n",
      "Epoch 472/1000\n",
      "80/80 [==============================] - 0s 20us/sample - loss: 107.6213 - val_loss: 11673.3105\n",
      "Epoch 473/1000\n",
      "80/80 [==============================] - 0s 18us/sample - loss: 107.1106 - val_loss: 11671.5996\n",
      "Epoch 474/1000\n",
      "80/80 [==============================] - 0s 23us/sample - loss: 106.7162 - val_loss: 11670.1426\n",
      "Epoch 475/1000\n",
      "80/80 [==============================] - 0s 19us/sample - loss: 106.3747 - val_loss: 11668.7852\n",
      "Epoch 476/1000\n",
      "80/80 [==============================] - 0s 17us/sample - loss: 105.8978 - val_loss: 11667.4873\n",
      "Epoch 477/1000\n",
      "80/80 [==============================] - 0s 18us/sample - loss: 105.5500 - val_loss: 11666.4258\n",
      "Epoch 478/1000\n",
      "80/80 [==============================] - 0s 18us/sample - loss: 105.1484 - val_loss: 11665.4424\n",
      "Epoch 479/1000\n",
      "80/80 [==============================] - 0s 36us/sample - loss: 104.7556 - val_loss: 11664.4443\n",
      "Epoch 480/1000\n",
      "80/80 [==============================] - 0s 44us/sample - loss: 104.3913 - val_loss: 11663.3906\n",
      "Epoch 481/1000\n",
      "80/80 [==============================] - 0s 61us/sample - loss: 104.0089 - val_loss: 11662.6074\n",
      "Epoch 482/1000\n",
      "80/80 [==============================] - 0s 53us/sample - loss: 103.6581 - val_loss: 11661.7842\n",
      "Epoch 483/1000\n",
      "80/80 [==============================] - 0s 283us/sample - loss: 103.3012 - val_loss: 11661.2598\n",
      "Epoch 484/1000\n",
      "80/80 [==============================] - 0s 26us/sample - loss: 102.9747 - val_loss: 11660.7432\n",
      "Epoch 485/1000\n",
      "80/80 [==============================] - 0s 33us/sample - loss: 102.6303 - val_loss: 11660.3955\n",
      "Epoch 486/1000\n",
      "80/80 [==============================] - 0s 46us/sample - loss: 102.2817 - val_loss: 11659.8008\n",
      "Epoch 487/1000\n",
      "80/80 [==============================] - 0s 24us/sample - loss: 101.9637 - val_loss: 11659.5322\n",
      "Epoch 488/1000\n",
      "80/80 [==============================] - 0s 21us/sample - loss: 101.6204 - val_loss: 11659.2207\n",
      "Epoch 489/1000\n",
      "80/80 [==============================] - 0s 24us/sample - loss: 101.2919 - val_loss: 11658.8652\n",
      "Epoch 490/1000\n",
      "80/80 [==============================] - 0s 36us/sample - loss: 101.0081 - val_loss: 11658.5107\n",
      "Epoch 491/1000\n",
      "80/80 [==============================] - 0s 33us/sample - loss: 100.6448 - val_loss: 11658.0645\n",
      "Epoch 492/1000\n",
      "80/80 [==============================] - 0s 19us/sample - loss: 100.3960 - val_loss: 11657.6016\n",
      "Epoch 493/1000\n",
      "80/80 [==============================] - 0s 17us/sample - loss: 100.1051 - val_loss: 11657.0244\n",
      "Epoch 494/1000\n",
      "80/80 [==============================] - 0s 18us/sample - loss: 99.7791 - val_loss: 11656.4004\n",
      "Epoch 495/1000\n",
      "80/80 [==============================] - 0s 30us/sample - loss: 99.4967 - val_loss: 11655.8574\n",
      "Epoch 496/1000\n",
      "80/80 [==============================] - 0s 36us/sample - loss: 99.2225 - val_loss: 11655.3262\n",
      "Epoch 497/1000\n",
      "80/80 [==============================] - 0s 32us/sample - loss: 98.9231 - val_loss: 11654.4980\n",
      "Epoch 498/1000\n",
      "80/80 [==============================] - 0s 20us/sample - loss: 98.6551 - val_loss: 11653.9590\n",
      "Epoch 499/1000\n",
      "80/80 [==============================] - 0s 20us/sample - loss: 98.4111 - val_loss: 11653.3408\n",
      "Epoch 500/1000\n",
      "80/80 [==============================] - 0s 18us/sample - loss: 98.1601 - val_loss: 11652.8906\n",
      "Epoch 501/1000\n",
      "80/80 [==============================] - 0s 17us/sample - loss: 97.8979 - val_loss: 11652.3145\n",
      "Epoch 502/1000\n",
      "80/80 [==============================] - 0s 18us/sample - loss: 97.6405 - val_loss: 11651.6973\n",
      "Epoch 503/1000\n",
      "80/80 [==============================] - 0s 15us/sample - loss: 97.4142 - val_loss: 11651.1113\n",
      "Epoch 504/1000\n",
      "80/80 [==============================] - 0s 16us/sample - loss: 97.1453 - val_loss: 11650.6816\n",
      "Epoch 505/1000\n",
      "80/80 [==============================] - 0s 18us/sample - loss: 96.9377 - val_loss: 11650.3467\n",
      "Epoch 506/1000\n",
      "80/80 [==============================] - 0s 16us/sample - loss: 96.6844 - val_loss: 11649.8975\n",
      "Epoch 507/1000\n",
      "80/80 [==============================] - 0s 18us/sample - loss: 96.4588 - val_loss: 11649.0732\n",
      "Epoch 508/1000\n",
      "80/80 [==============================] - 0s 16us/sample - loss: 96.2518 - val_loss: 11648.3643\n",
      "Epoch 509/1000\n",
      "80/80 [==============================] - 0s 14us/sample - loss: 96.0210 - val_loss: 11647.6816\n",
      "Epoch 510/1000\n",
      "80/80 [==============================] - 0s 14us/sample - loss: 95.7886 - val_loss: 11647.0225\n",
      "Epoch 511/1000\n",
      "80/80 [==============================] - 0s 16us/sample - loss: 95.5623 - val_loss: 11646.4238\n",
      "Epoch 512/1000\n",
      "80/80 [==============================] - 0s 26us/sample - loss: 95.3692 - val_loss: 11645.7422\n",
      "Epoch 513/1000\n",
      "80/80 [==============================] - 0s 26us/sample - loss: 95.1350 - val_loss: 11644.9590\n",
      "Epoch 514/1000\n",
      "80/80 [==============================] - 0s 21us/sample - loss: 94.9440 - val_loss: 11644.3145\n",
      "Epoch 515/1000\n",
      "80/80 [==============================] - 0s 100us/sample - loss: 94.7246 - val_loss: 11643.6953\n",
      "Epoch 516/1000\n",
      "80/80 [==============================] - 0s 128us/sample - loss: 94.5183 - val_loss: 11643.0488\n",
      "Epoch 517/1000\n",
      "80/80 [==============================] - 0s 37us/sample - loss: 94.3248 - val_loss: 11642.5723\n",
      "Epoch 518/1000\n",
      "80/80 [==============================] - 0s 21us/sample - loss: 94.1105 - val_loss: 11642.1465\n",
      "Epoch 519/1000\n",
      "80/80 [==============================] - 0s 20us/sample - loss: 93.9219 - val_loss: 11641.7734\n",
      "Epoch 520/1000\n",
      "80/80 [==============================] - 0s 19us/sample - loss: 93.7199 - val_loss: 11641.2363\n",
      "Epoch 521/1000\n",
      "80/80 [==============================] - 0s 21us/sample - loss: 93.5627 - val_loss: 11640.6279\n",
      "Epoch 522/1000\n",
      "80/80 [==============================] - 0s 17us/sample - loss: 93.3733 - val_loss: 11639.8828\n",
      "Epoch 523/1000\n",
      "80/80 [==============================] - 0s 18us/sample - loss: 93.1841 - val_loss: 11639.1631\n",
      "Epoch 524/1000\n",
      "80/80 [==============================] - 0s 19us/sample - loss: 93.0126 - val_loss: 11638.5781\n",
      "Epoch 525/1000\n",
      "80/80 [==============================] - 0s 18us/sample - loss: 92.8221 - val_loss: 11637.9316\n",
      "Epoch 526/1000\n",
      "80/80 [==============================] - 0s 23us/sample - loss: 92.6714 - val_loss: 11637.2988\n",
      "Epoch 527/1000\n",
      "80/80 [==============================] - 0s 22us/sample - loss: 92.5067 - val_loss: 11636.6553\n",
      "Epoch 528/1000\n",
      "80/80 [==============================] - 0s 19us/sample - loss: 92.3520 - val_loss: 11636.0996\n",
      "Epoch 529/1000\n",
      "80/80 [==============================] - 0s 74us/sample - loss: 92.1746 - val_loss: 11635.5176\n",
      "Epoch 530/1000\n",
      "80/80 [==============================] - 0s 41us/sample - loss: 92.0279 - val_loss: 11634.8164\n",
      "Epoch 531/1000\n",
      "80/80 [==============================] - 0s 20us/sample - loss: 91.8737 - val_loss: 11634.0244\n",
      "Epoch 532/1000\n",
      "80/80 [==============================] - 0s 20us/sample - loss: 91.7111 - val_loss: 11633.2539\n",
      "Epoch 533/1000\n",
      "80/80 [==============================] - 0s 22us/sample - loss: 91.5752 - val_loss: 11632.5371\n",
      "Epoch 534/1000\n",
      "80/80 [==============================] - 0s 18us/sample - loss: 91.4318 - val_loss: 11631.8438\n",
      "Epoch 535/1000\n",
      "80/80 [==============================] - 0s 18us/sample - loss: 91.2808 - val_loss: 11631.2129\n",
      "Epoch 536/1000\n",
      "80/80 [==============================] - 0s 16us/sample - loss: 91.1355 - val_loss: 11630.5713\n",
      "Epoch 537/1000\n",
      "80/80 [==============================] - 0s 16us/sample - loss: 90.9793 - val_loss: 11629.8408\n",
      "Epoch 538/1000\n",
      "80/80 [==============================] - 0s 16us/sample - loss: 90.8477 - val_loss: 11629.0996\n",
      "Epoch 539/1000\n",
      "80/80 [==============================] - 0s 15us/sample - loss: 90.7134 - val_loss: 11628.4521\n",
      "Epoch 540/1000\n",
      "80/80 [==============================] - 0s 16us/sample - loss: 90.5616 - val_loss: 11627.8477\n",
      "Epoch 541/1000\n",
      "80/80 [==============================] - 0s 14us/sample - loss: 90.4338 - val_loss: 11627.1367\n",
      "Epoch 542/1000\n",
      "80/80 [==============================] - 0s 14us/sample - loss: 90.2941 - val_loss: 11626.4404\n",
      "Epoch 543/1000\n",
      "80/80 [==============================] - 0s 13us/sample - loss: 90.1554 - val_loss: 11625.7793\n",
      "Epoch 544/1000\n",
      "80/80 [==============================] - 0s 15us/sample - loss: 90.0192 - val_loss: 11625.1689\n",
      "Epoch 545/1000\n",
      "80/80 [==============================] - 0s 24us/sample - loss: 89.8796 - val_loss: 11624.5605\n",
      "Epoch 546/1000\n",
      "80/80 [==============================] - 0s 26us/sample - loss: 89.7604 - val_loss: 11623.9482\n",
      "Epoch 547/1000\n",
      "80/80 [==============================] - 0s 26us/sample - loss: 89.6182 - val_loss: 11623.4424\n",
      "Epoch 548/1000\n",
      "80/80 [==============================] - 0s 21us/sample - loss: 89.5147 - val_loss: 11622.9619\n",
      "Epoch 549/1000\n",
      "80/80 [==============================] - 0s 18us/sample - loss: 89.3695 - val_loss: 11622.2910\n",
      "Epoch 550/1000\n",
      "80/80 [==============================] - 0s 19us/sample - loss: 89.2453 - val_loss: 11621.8047\n",
      "Epoch 551/1000\n",
      "80/80 [==============================] - 0s 19us/sample - loss: 89.1259 - val_loss: 11621.2012\n",
      "Epoch 552/1000\n",
      "80/80 [==============================] - 0s 19us/sample - loss: 89.0116 - val_loss: 11620.5029\n",
      "Epoch 553/1000\n",
      "80/80 [==============================] - 0s 17us/sample - loss: 88.8925 - val_loss: 11619.8213\n",
      "Epoch 554/1000\n",
      "80/80 [==============================] - 0s 21us/sample - loss: 88.7807 - val_loss: 11619.1943\n",
      "Epoch 555/1000\n",
      "80/80 [==============================] - 0s 19us/sample - loss: 88.6535 - val_loss: 11618.6777\n",
      "Epoch 556/1000\n",
      "80/80 [==============================] - 0s 19us/sample - loss: 88.5475 - val_loss: 11618.2285\n",
      "Epoch 557/1000\n",
      "80/80 [==============================] - 0s 14us/sample - loss: 88.4424 - val_loss: 11617.6895\n",
      "Epoch 558/1000\n",
      "80/80 [==============================] - 0s 16us/sample - loss: 88.3246 - val_loss: 11617.1729\n",
      "Epoch 559/1000\n",
      "80/80 [==============================] - 0s 16us/sample - loss: 88.2166 - val_loss: 11616.5469\n",
      "Epoch 560/1000\n",
      "80/80 [==============================] - 0s 17us/sample - loss: 88.1272 - val_loss: 11615.9473\n",
      "Epoch 561/1000\n",
      "80/80 [==============================] - 0s 17us/sample - loss: 88.0184 - val_loss: 11615.4512\n",
      "Epoch 562/1000\n",
      "80/80 [==============================] - 0s 17us/sample - loss: 87.9024 - val_loss: 11614.9043\n",
      "Epoch 563/1000\n",
      "80/80 [==============================] - 0s 16us/sample - loss: 87.8168 - val_loss: 11614.3467\n",
      "Epoch 564/1000\n",
      "80/80 [==============================] - 0s 17us/sample - loss: 87.7085 - val_loss: 11613.9268\n",
      "Epoch 565/1000\n",
      "80/80 [==============================] - 0s 20us/sample - loss: 87.6164 - val_loss: 11613.4980\n",
      "Epoch 566/1000\n",
      "80/80 [==============================] - 0s 24us/sample - loss: 87.5021 - val_loss: 11613.0908\n",
      "Epoch 567/1000\n",
      "80/80 [==============================] - 0s 51us/sample - loss: 87.4165 - val_loss: 11612.6279\n",
      "Epoch 568/1000\n",
      "80/80 [==============================] - 0s 103us/sample - loss: 87.3243 - val_loss: 11612.2324\n",
      "Epoch 569/1000\n",
      "80/80 [==============================] - 0s 64us/sample - loss: 87.2328 - val_loss: 11611.8691\n",
      "Epoch 570/1000\n",
      "80/80 [==============================] - 0s 23us/sample - loss: 87.1439 - val_loss: 11611.2920\n",
      "Epoch 571/1000\n",
      "80/80 [==============================] - 0s 22us/sample - loss: 87.0509 - val_loss: 11610.5850\n",
      "Epoch 572/1000\n",
      "80/80 [==============================] - 0s 16us/sample - loss: 86.9639 - val_loss: 11609.9062\n",
      "Epoch 573/1000\n",
      "80/80 [==============================] - 0s 20us/sample - loss: 86.8696 - val_loss: 11609.3027\n",
      "Epoch 574/1000\n",
      "80/80 [==============================] - 0s 17us/sample - loss: 86.7907 - val_loss: 11608.6592\n",
      "Epoch 575/1000\n",
      "80/80 [==============================] - 0s 19us/sample - loss: 86.7039 - val_loss: 11607.9980\n",
      "Epoch 576/1000\n",
      "80/80 [==============================] - 0s 23us/sample - loss: 86.6243 - val_loss: 11607.3965\n",
      "Epoch 577/1000\n",
      "80/80 [==============================] - 0s 47us/sample - loss: 86.5389 - val_loss: 11606.7480\n",
      "Epoch 578/1000\n",
      "80/80 [==============================] - 0s 23us/sample - loss: 86.4636 - val_loss: 11606.2012\n",
      "Epoch 579/1000\n",
      "80/80 [==============================] - 0s 23us/sample - loss: 86.3790 - val_loss: 11605.6338\n",
      "Epoch 580/1000\n",
      "80/80 [==============================] - 0s 20us/sample - loss: 86.2867 - val_loss: 11605.0889\n",
      "Epoch 581/1000\n",
      "80/80 [==============================] - 0s 20us/sample - loss: 86.1952 - val_loss: 11604.5762\n",
      "Epoch 582/1000\n",
      "80/80 [==============================] - 0s 19us/sample - loss: 86.1187 - val_loss: 11604.0410\n",
      "Epoch 583/1000\n",
      "80/80 [==============================] - 0s 18us/sample - loss: 86.0211 - val_loss: 11603.4102\n",
      "Epoch 584/1000\n",
      "80/80 [==============================] - 0s 19us/sample - loss: 85.9470 - val_loss: 11602.6475\n",
      "Epoch 585/1000\n",
      "80/80 [==============================] - 0s 16us/sample - loss: 85.8574 - val_loss: 11601.9131\n",
      "Epoch 586/1000\n",
      "80/80 [==============================] - 0s 16us/sample - loss: 85.7778 - val_loss: 11601.1543\n",
      "Epoch 587/1000\n",
      "80/80 [==============================] - 0s 16us/sample - loss: 85.6856 - val_loss: 11600.4277\n",
      "Epoch 588/1000\n",
      "80/80 [==============================] - 0s 15us/sample - loss: 85.6200 - val_loss: 11599.6934\n",
      "Epoch 589/1000\n",
      "80/80 [==============================] - 0s 14us/sample - loss: 85.5356 - val_loss: 11598.9365\n",
      "Epoch 590/1000\n",
      "80/80 [==============================] - 0s 15us/sample - loss: 85.4550 - val_loss: 11598.1699\n",
      "Epoch 591/1000\n",
      "80/80 [==============================] - 0s 15us/sample - loss: 85.3784 - val_loss: 11597.4316\n",
      "Epoch 592/1000\n",
      "80/80 [==============================] - 0s 17us/sample - loss: 85.3100 - val_loss: 11596.6387\n",
      "Epoch 593/1000\n",
      "80/80 [==============================] - 0s 16us/sample - loss: 85.2376 - val_loss: 11595.8496\n",
      "Epoch 594/1000\n",
      "80/80 [==============================] - 0s 16us/sample - loss: 85.1689 - val_loss: 11595.1367\n",
      "Epoch 595/1000\n",
      "80/80 [==============================] - 0s 17us/sample - loss: 85.0936 - val_loss: 11594.4561\n",
      "Epoch 596/1000\n",
      "80/80 [==============================] - 0s 19us/sample - loss: 85.0154 - val_loss: 11593.7803\n",
      "Epoch 597/1000\n",
      "80/80 [==============================] - 0s 19us/sample - loss: 84.9513 - val_loss: 11593.0605\n",
      "Epoch 598/1000\n",
      "80/80 [==============================] - 0s 31us/sample - loss: 84.8920 - val_loss: 11592.3496\n",
      "Epoch 599/1000\n",
      "80/80 [==============================] - 0s 38us/sample - loss: 84.8150 - val_loss: 11591.6387\n",
      "Epoch 600/1000\n",
      "80/80 [==============================] - 0s 22us/sample - loss: 84.7632 - val_loss: 11590.9131\n",
      "Epoch 601/1000\n",
      "80/80 [==============================] - 0s 16us/sample - loss: 84.6952 - val_loss: 11590.3340\n",
      "Epoch 602/1000\n",
      "80/80 [==============================] - 0s 17us/sample - loss: 84.6350 - val_loss: 11589.6611\n",
      "Epoch 603/1000\n",
      "80/80 [==============================] - 0s 14us/sample - loss: 84.5696 - val_loss: 11589.0391\n",
      "Epoch 604/1000\n",
      "80/80 [==============================] - 0s 15us/sample - loss: 84.5099 - val_loss: 11588.4434\n",
      "Epoch 605/1000\n",
      "80/80 [==============================] - 0s 14us/sample - loss: 84.4548 - val_loss: 11587.7910\n",
      "Epoch 606/1000\n",
      "80/80 [==============================] - 0s 16us/sample - loss: 84.3911 - val_loss: 11587.0488\n",
      "Epoch 607/1000\n",
      "80/80 [==============================] - 0s 17us/sample - loss: 84.3409 - val_loss: 11586.1846\n",
      "Epoch 608/1000\n",
      "80/80 [==============================] - 0s 27us/sample - loss: 84.2780 - val_loss: 11585.4023\n",
      "Epoch 609/1000\n",
      "80/80 [==============================] - 0s 21us/sample - loss: 84.2361 - val_loss: 11584.4902\n",
      "Epoch 610/1000\n",
      "80/80 [==============================] - 0s 20us/sample - loss: 84.1785 - val_loss: 11583.6455\n",
      "Epoch 611/1000\n",
      "80/80 [==============================] - 0s 17us/sample - loss: 84.1236 - val_loss: 11582.8926\n",
      "Epoch 612/1000\n",
      "80/80 [==============================] - 0s 16us/sample - loss: 84.0639 - val_loss: 11582.1426\n",
      "Epoch 613/1000\n",
      "80/80 [==============================] - 0s 16us/sample - loss: 84.0177 - val_loss: 11581.3125\n",
      "Epoch 614/1000\n",
      "80/80 [==============================] - 0s 17us/sample - loss: 83.9649 - val_loss: 11580.4316\n",
      "Epoch 615/1000\n",
      "80/80 [==============================] - 0s 17us/sample - loss: 83.9174 - val_loss: 11579.6143\n",
      "Epoch 616/1000\n",
      "80/80 [==============================] - 0s 17us/sample - loss: 83.8652 - val_loss: 11578.8330\n",
      "Epoch 617/1000\n",
      "80/80 [==============================] - 0s 14us/sample - loss: 83.8200 - val_loss: 11578.1230\n",
      "Epoch 618/1000\n",
      "80/80 [==============================] - 0s 16us/sample - loss: 83.7640 - val_loss: 11577.4639\n",
      "Epoch 619/1000\n",
      "80/80 [==============================] - 0s 14us/sample - loss: 83.7129 - val_loss: 11576.8359\n",
      "Epoch 620/1000\n",
      "80/80 [==============================] - 0s 18us/sample - loss: 83.6624 - val_loss: 11576.1465\n",
      "Epoch 621/1000\n",
      "80/80 [==============================] - 0s 16us/sample - loss: 83.6257 - val_loss: 11575.3965\n",
      "Epoch 622/1000\n",
      "80/80 [==============================] - 0s 14us/sample - loss: 83.5661 - val_loss: 11574.7559\n",
      "Epoch 623/1000\n",
      "80/80 [==============================] - 0s 14us/sample - loss: 83.5282 - val_loss: 11574.0918\n",
      "Epoch 624/1000\n",
      "80/80 [==============================] - 0s 14us/sample - loss: 83.4823 - val_loss: 11573.4639\n",
      "Epoch 625/1000\n",
      "80/80 [==============================] - 0s 14us/sample - loss: 83.4252 - val_loss: 11572.8809\n",
      "Epoch 626/1000\n",
      "80/80 [==============================] - 0s 15us/sample - loss: 83.3806 - val_loss: 11572.1328\n",
      "Epoch 627/1000\n",
      "80/80 [==============================] - 0s 14us/sample - loss: 83.3377 - val_loss: 11571.3262\n",
      "Epoch 628/1000\n",
      "80/80 [==============================] - 0s 17us/sample - loss: 83.2904 - val_loss: 11570.5332\n",
      "Epoch 629/1000\n",
      "80/80 [==============================] - 0s 17us/sample - loss: 83.2492 - val_loss: 11569.7422\n",
      "Epoch 630/1000\n",
      "80/80 [==============================] - 0s 31us/sample - loss: 83.1990 - val_loss: 11569.0449\n",
      "Epoch 631/1000\n",
      "80/80 [==============================] - 0s 32us/sample - loss: 83.1616 - val_loss: 11568.4111\n",
      "Epoch 632/1000\n",
      "80/80 [==============================] - 0s 30us/sample - loss: 83.1153 - val_loss: 11567.7520\n",
      "Epoch 633/1000\n",
      "80/80 [==============================] - 0s 24us/sample - loss: 83.0777 - val_loss: 11567.0762\n",
      "Epoch 634/1000\n",
      "80/80 [==============================] - 0s 16us/sample - loss: 83.0254 - val_loss: 11566.4619\n",
      "Epoch 635/1000\n",
      "80/80 [==============================] - 0s 20us/sample - loss: 82.9939 - val_loss: 11565.7500\n",
      "Epoch 636/1000\n",
      "80/80 [==============================] - 0s 18us/sample - loss: 82.9508 - val_loss: 11565.0908\n",
      "Epoch 637/1000\n",
      "80/80 [==============================] - 0s 132us/sample - loss: 82.9118 - val_loss: 11564.3486\n",
      "Epoch 638/1000\n",
      "80/80 [==============================] - 0s 140us/sample - loss: 82.8678 - val_loss: 11563.6182\n",
      "Epoch 639/1000\n",
      "80/80 [==============================] - 0s 26us/sample - loss: 82.8365 - val_loss: 11562.8252\n",
      "Epoch 640/1000\n",
      "80/80 [==============================] - 0s 21us/sample - loss: 82.7951 - val_loss: 11562.0654\n",
      "Epoch 641/1000\n",
      "80/80 [==============================] - 0s 25us/sample - loss: 82.7567 - val_loss: 11561.3887\n",
      "Epoch 642/1000\n",
      "80/80 [==============================] - 0s 22us/sample - loss: 82.7175 - val_loss: 11560.8457\n",
      "Epoch 643/1000\n",
      "80/80 [==============================] - 0s 44us/sample - loss: 82.6807 - val_loss: 11560.2871\n",
      "Epoch 644/1000\n",
      "80/80 [==============================] - 0s 38us/sample - loss: 82.6484 - val_loss: 11559.6934\n",
      "Epoch 645/1000\n",
      "80/80 [==============================] - 0s 21us/sample - loss: 82.6127 - val_loss: 11559.1133\n",
      "Epoch 646/1000\n",
      "80/80 [==============================] - 0s 19us/sample - loss: 82.5753 - val_loss: 11558.5449\n",
      "Epoch 647/1000\n",
      "80/80 [==============================] - 0s 18us/sample - loss: 82.5398 - val_loss: 11558.0020\n",
      "Epoch 648/1000\n",
      "80/80 [==============================] - 0s 17us/sample - loss: 82.5078 - val_loss: 11557.3496\n",
      "Epoch 649/1000\n",
      "80/80 [==============================] - 0s 19us/sample - loss: 82.4733 - val_loss: 11556.6436\n",
      "Epoch 650/1000\n",
      "80/80 [==============================] - 0s 23us/sample - loss: 82.4361 - val_loss: 11555.9863\n",
      "Epoch 651/1000\n",
      "80/80 [==============================] - 0s 18us/sample - loss: 82.4103 - val_loss: 11555.2793\n",
      "Epoch 652/1000\n",
      "80/80 [==============================] - 0s 15us/sample - loss: 82.3686 - val_loss: 11554.6230\n",
      "Epoch 653/1000\n",
      "80/80 [==============================] - 0s 15us/sample - loss: 82.3432 - val_loss: 11553.9521\n",
      "Epoch 654/1000\n",
      "80/80 [==============================] - 0s 16us/sample - loss: 82.3096 - val_loss: 11553.3301\n",
      "Epoch 655/1000\n",
      "80/80 [==============================] - 0s 25us/sample - loss: 82.2729 - val_loss: 11552.7549\n",
      "Epoch 656/1000\n",
      "80/80 [==============================] - 0s 37us/sample - loss: 82.2404 - val_loss: 11552.1670\n",
      "Epoch 657/1000\n",
      "80/80 [==============================] - 0s 27us/sample - loss: 82.2136 - val_loss: 11551.5547\n",
      "Epoch 658/1000\n",
      "80/80 [==============================] - 0s 23us/sample - loss: 82.1839 - val_loss: 11550.9600\n",
      "Epoch 659/1000\n",
      "80/80 [==============================] - 0s 24us/sample - loss: 82.1449 - val_loss: 11550.3955\n",
      "Epoch 660/1000\n",
      "80/80 [==============================] - 0s 19us/sample - loss: 82.1194 - val_loss: 11549.6992\n",
      "Epoch 661/1000\n",
      "80/80 [==============================] - 0s 17us/sample - loss: 82.0859 - val_loss: 11549.0078\n",
      "Epoch 662/1000\n",
      "80/80 [==============================] - 0s 14us/sample - loss: 82.0569 - val_loss: 11548.2705\n",
      "Epoch 663/1000\n",
      "80/80 [==============================] - 0s 16us/sample - loss: 82.0190 - val_loss: 11547.6211\n",
      "Epoch 664/1000\n",
      "80/80 [==============================] - 0s 17us/sample - loss: 81.9964 - val_loss: 11546.9258\n",
      "Epoch 665/1000\n",
      "80/80 [==============================] - 0s 18us/sample - loss: 81.9659 - val_loss: 11546.2236\n",
      "Epoch 666/1000\n",
      "80/80 [==============================] - 0s 16us/sample - loss: 81.9368 - val_loss: 11545.5479\n",
      "Epoch 667/1000\n",
      "80/80 [==============================] - 0s 14us/sample - loss: 81.9048 - val_loss: 11544.8965\n",
      "Epoch 668/1000\n",
      "80/80 [==============================] - 0s 15us/sample - loss: 81.8779 - val_loss: 11544.2080\n",
      "Epoch 669/1000\n",
      "80/80 [==============================] - 0s 26us/sample - loss: 81.8512 - val_loss: 11543.5586\n",
      "Epoch 670/1000\n",
      "80/80 [==============================] - 0s 24us/sample - loss: 81.8231 - val_loss: 11542.9277\n",
      "Epoch 671/1000\n",
      "80/80 [==============================] - 0s 21us/sample - loss: 81.7937 - val_loss: 11542.3652\n",
      "Epoch 672/1000\n",
      "80/80 [==============================] - 0s 19us/sample - loss: 81.7676 - val_loss: 11541.8311\n",
      "Epoch 673/1000\n",
      "80/80 [==============================] - 0s 16us/sample - loss: 81.7372 - val_loss: 11541.3145\n",
      "Epoch 674/1000\n",
      "80/80 [==============================] - 0s 18us/sample - loss: 81.7163 - val_loss: 11540.7578\n",
      "Epoch 675/1000\n",
      "80/80 [==============================] - 0s 18us/sample - loss: 81.6885 - val_loss: 11540.1465\n",
      "Epoch 676/1000\n",
      "80/80 [==============================] - 0s 20us/sample - loss: 81.6573 - val_loss: 11539.4922\n",
      "Epoch 677/1000\n",
      "80/80 [==============================] - 0s 40us/sample - loss: 81.6343 - val_loss: 11538.8408\n",
      "Epoch 678/1000\n",
      "80/80 [==============================] - 0s 20us/sample - loss: 81.6045 - val_loss: 11538.2432\n",
      "Epoch 679/1000\n",
      "80/80 [==============================] - 0s 19us/sample - loss: 81.5828 - val_loss: 11537.6074\n",
      "Epoch 680/1000\n",
      "80/80 [==============================] - 0s 18us/sample - loss: 81.5565 - val_loss: 11536.9238\n",
      "Epoch 681/1000\n",
      "80/80 [==============================] - 0s 17us/sample - loss: 81.5281 - val_loss: 11536.2871\n",
      "Epoch 682/1000\n",
      "80/80 [==============================] - 0s 24us/sample - loss: 81.5041 - val_loss: 11535.6738\n",
      "Epoch 683/1000\n",
      "80/80 [==============================] - 0s 20us/sample - loss: 81.4774 - val_loss: 11535.0732\n",
      "Epoch 684/1000\n",
      "80/80 [==============================] - 0s 22us/sample - loss: 81.4541 - val_loss: 11534.4453\n",
      "Epoch 685/1000\n",
      "80/80 [==============================] - 0s 18us/sample - loss: 81.4293 - val_loss: 11533.8584\n",
      "Epoch 686/1000\n",
      "80/80 [==============================] - 0s 15us/sample - loss: 81.4054 - val_loss: 11533.2705\n",
      "Epoch 687/1000\n",
      "80/80 [==============================] - 0s 15us/sample - loss: 81.3827 - val_loss: 11532.6982\n",
      "Epoch 688/1000\n",
      "80/80 [==============================] - 0s 15us/sample - loss: 81.3593 - val_loss: 11532.1504\n",
      "Epoch 689/1000\n",
      "80/80 [==============================] - 0s 14us/sample - loss: 81.3312 - val_loss: 11531.5977\n",
      "Epoch 690/1000\n",
      "80/80 [==============================] - 0s 14us/sample - loss: 81.3084 - val_loss: 11531.0078\n",
      "Epoch 691/1000\n",
      "80/80 [==============================] - 0s 14us/sample - loss: 81.2844 - val_loss: 11530.3789\n",
      "Epoch 692/1000\n",
      "80/80 [==============================] - 0s 14us/sample - loss: 81.2652 - val_loss: 11529.6699\n",
      "Epoch 693/1000\n",
      "80/80 [==============================] - 0s 15us/sample - loss: 81.2407 - val_loss: 11528.9766\n",
      "Epoch 694/1000\n",
      "80/80 [==============================] - 0s 14us/sample - loss: 81.2096 - val_loss: 11528.3848\n",
      "Epoch 695/1000\n",
      "80/80 [==============================] - 0s 14us/sample - loss: 81.1924 - val_loss: 11527.7227\n",
      "Epoch 696/1000\n",
      "80/80 [==============================] - 0s 15us/sample - loss: 81.1659 - val_loss: 11527.0762\n",
      "Epoch 697/1000\n",
      "80/80 [==============================] - 0s 16us/sample - loss: 81.1426 - val_loss: 11526.4111\n",
      "Epoch 698/1000\n",
      "80/80 [==============================] - 0s 17us/sample - loss: 81.1206 - val_loss: 11525.7627\n",
      "Epoch 699/1000\n",
      "80/80 [==============================] - 0s 25us/sample - loss: 81.0917 - val_loss: 11525.1758\n",
      "Epoch 700/1000\n",
      "80/80 [==============================] - 0s 22us/sample - loss: 81.0702 - val_loss: 11524.5625\n",
      "Epoch 701/1000\n",
      "80/80 [==============================] - 0s 22us/sample - loss: 81.0518 - val_loss: 11523.8965\n",
      "Epoch 702/1000\n",
      "80/80 [==============================] - 0s 17us/sample - loss: 81.0297 - val_loss: 11523.2314\n",
      "Epoch 703/1000\n",
      "80/80 [==============================] - 0s 33us/sample - loss: 81.0064 - val_loss: 11522.6152\n",
      "Epoch 704/1000\n",
      "80/80 [==============================] - 0s 242us/sample - loss: 80.9774 - val_loss: 11522.0361\n",
      "Epoch 705/1000\n",
      "80/80 [==============================] - 0s 35us/sample - loss: 80.9611 - val_loss: 11521.4512\n",
      "Epoch 706/1000\n",
      "80/80 [==============================] - 0s 25us/sample - loss: 80.9394 - val_loss: 11520.8779\n",
      "Epoch 707/1000\n",
      "80/80 [==============================] - 0s 21us/sample - loss: 80.9160 - val_loss: 11520.3262\n",
      "Epoch 708/1000\n",
      "80/80 [==============================] - 0s 21us/sample - loss: 80.8909 - val_loss: 11519.7871\n",
      "Epoch 709/1000\n",
      "80/80 [==============================] - 0s 22us/sample - loss: 80.8689 - val_loss: 11519.1924\n",
      "Epoch 710/1000\n",
      "80/80 [==============================] - 0s 26us/sample - loss: 80.8538 - val_loss: 11518.5918\n",
      "Epoch 711/1000\n",
      "80/80 [==============================] - 0s 28us/sample - loss: 80.8311 - val_loss: 11517.9561\n",
      "Epoch 712/1000\n",
      "80/80 [==============================] - 0s 24us/sample - loss: 80.8079 - val_loss: 11517.3418\n",
      "Epoch 713/1000\n",
      "80/80 [==============================] - 0s 22us/sample - loss: 80.7885 - val_loss: 11516.6807\n",
      "Epoch 714/1000\n",
      "80/80 [==============================] - 0s 15us/sample - loss: 80.7616 - val_loss: 11516.0547\n",
      "Epoch 715/1000\n",
      "80/80 [==============================] - 0s 15us/sample - loss: 80.7483 - val_loss: 11515.3770\n",
      "Epoch 716/1000\n",
      "80/80 [==============================] - 0s 16us/sample - loss: 80.7275 - val_loss: 11514.7656\n",
      "Epoch 717/1000\n",
      "80/80 [==============================] - 0s 17us/sample - loss: 80.7067 - val_loss: 11514.1895\n",
      "Epoch 718/1000\n",
      "80/80 [==============================] - 0s 14us/sample - loss: 80.6866 - val_loss: 11513.5498\n",
      "Epoch 719/1000\n",
      "80/80 [==============================] - 0s 15us/sample - loss: 80.6670 - val_loss: 11512.9668\n",
      "Epoch 720/1000\n",
      "80/80 [==============================] - 0s 15us/sample - loss: 80.6432 - val_loss: 11512.4248\n",
      "Epoch 721/1000\n",
      "80/80 [==============================] - 0s 16us/sample - loss: 80.6250 - val_loss: 11511.7998\n",
      "Epoch 722/1000\n",
      "80/80 [==============================] - 0s 15us/sample - loss: 80.6114 - val_loss: 11511.1758\n",
      "Epoch 723/1000\n",
      "80/80 [==============================] - 0s 24us/sample - loss: 80.5901 - val_loss: 11510.5576\n",
      "Epoch 724/1000\n",
      "80/80 [==============================] - 0s 27us/sample - loss: 80.5691 - val_loss: 11509.9219\n",
      "Epoch 725/1000\n",
      "80/80 [==============================] - 0s 15us/sample - loss: 80.5455 - val_loss: 11509.3027\n",
      "Epoch 726/1000\n",
      "80/80 [==============================] - 0s 16us/sample - loss: 80.5318 - val_loss: 11508.6904\n",
      "Epoch 727/1000\n",
      "80/80 [==============================] - 0s 17us/sample - loss: 80.5064 - val_loss: 11508.0918\n",
      "Epoch 728/1000\n",
      "80/80 [==============================] - 0s 14us/sample - loss: 80.4938 - val_loss: 11507.4580\n",
      "Epoch 729/1000\n",
      "80/80 [==============================] - 0s 16us/sample - loss: 80.4732 - val_loss: 11506.8965\n",
      "Epoch 730/1000\n",
      "80/80 [==============================] - 0s 15us/sample - loss: 80.4541 - val_loss: 11506.3584\n",
      "Epoch 731/1000\n",
      "80/80 [==============================] - 0s 14us/sample - loss: 80.4348 - val_loss: 11505.8447\n",
      "Epoch 732/1000\n",
      "80/80 [==============================] - 0s 20us/sample - loss: 80.4103 - val_loss: 11505.3438\n",
      "Epoch 733/1000\n",
      "80/80 [==============================] - 0s 15us/sample - loss: 80.3945 - val_loss: 11504.4814\n",
      "Epoch 734/1000\n",
      "80/80 [==============================] - 0s 14us/sample - loss: 80.3831 - val_loss: 11503.3916\n",
      "Epoch 735/1000\n",
      "80/80 [==============================] - 0s 16us/sample - loss: 80.3671 - val_loss: 11502.4463\n",
      "Epoch 736/1000\n",
      "80/80 [==============================] - 0s 15us/sample - loss: 80.3445 - val_loss: 11501.6123\n",
      "Epoch 737/1000\n",
      "80/80 [==============================] - 0s 86us/sample - loss: 80.3282 - val_loss: 11500.8311\n",
      "Epoch 738/1000\n",
      "80/80 [==============================] - 0s 21us/sample - loss: 80.3112 - val_loss: 11500.1172\n",
      "Epoch 739/1000\n",
      "80/80 [==============================] - 0s 21us/sample - loss: 80.2932 - val_loss: 11499.3809\n",
      "Epoch 740/1000\n",
      "80/80 [==============================] - 0s 18us/sample - loss: 80.2804 - val_loss: 11498.6689\n",
      "Epoch 741/1000\n",
      "80/80 [==============================] - 0s 21us/sample - loss: 80.2603 - val_loss: 11498.0342\n",
      "Epoch 742/1000\n",
      "80/80 [==============================] - 0s 19us/sample - loss: 80.2407 - val_loss: 11497.4082\n",
      "Epoch 743/1000\n",
      "80/80 [==============================] - 0s 18us/sample - loss: 80.2209 - val_loss: 11496.8223\n",
      "Epoch 744/1000\n",
      "80/80 [==============================] - 0s 17us/sample - loss: 80.1961 - val_loss: 11496.2578\n",
      "Epoch 745/1000\n",
      "80/80 [==============================] - 0s 16us/sample - loss: 80.1844 - val_loss: 11495.6816\n",
      "Epoch 746/1000\n",
      "80/80 [==============================] - 0s 30us/sample - loss: 80.1589 - val_loss: 11495.1387\n",
      "Epoch 747/1000\n",
      "80/80 [==============================] - 0s 20us/sample - loss: 80.1461 - val_loss: 11494.5732\n",
      "Epoch 748/1000\n",
      "80/80 [==============================] - 0s 25us/sample - loss: 80.1269 - val_loss: 11494.0195\n",
      "Epoch 749/1000\n",
      "80/80 [==============================] - 0s 26us/sample - loss: 80.1013 - val_loss: 11493.5068\n",
      "Epoch 750/1000\n",
      "80/80 [==============================] - 0s 18us/sample - loss: 80.0839 - val_loss: 11492.9434\n",
      "Epoch 751/1000\n",
      "80/80 [==============================] - 0s 14us/sample - loss: 80.0645 - val_loss: 11492.3613\n",
      "Epoch 752/1000\n",
      "80/80 [==============================] - 0s 15us/sample - loss: 80.0520 - val_loss: 11491.7871\n",
      "Epoch 753/1000\n",
      "80/80 [==============================] - 0s 14us/sample - loss: 80.0320 - val_loss: 11491.3096\n",
      "Epoch 754/1000\n",
      "80/80 [==============================] - 0s 17us/sample - loss: 80.0117 - val_loss: 11490.8457\n",
      "Epoch 755/1000\n",
      "80/80 [==============================] - 0s 18us/sample - loss: 79.9880 - val_loss: 11490.3906\n",
      "Epoch 756/1000\n",
      "80/80 [==============================] - 0s 16us/sample - loss: 79.9700 - val_loss: 11489.8955\n",
      "Epoch 757/1000\n",
      "80/80 [==============================] - 0s 15us/sample - loss: 79.9581 - val_loss: 11489.3926\n",
      "Epoch 758/1000\n",
      "80/80 [==============================] - 0s 15us/sample - loss: 79.9385 - val_loss: 11488.9355\n",
      "Epoch 759/1000\n",
      "80/80 [==============================] - 0s 14us/sample - loss: 79.9143 - val_loss: 11488.5098\n",
      "Epoch 760/1000\n",
      "80/80 [==============================] - 0s 17us/sample - loss: 79.8960 - val_loss: 11488.0312\n",
      "Epoch 761/1000\n",
      "80/80 [==============================] - 0s 15us/sample - loss: 79.8846 - val_loss: 11487.5273\n",
      "Epoch 762/1000\n",
      "80/80 [==============================] - 0s 18us/sample - loss: 79.8658 - val_loss: 11486.9980\n",
      "Epoch 763/1000\n",
      "80/80 [==============================] - 0s 16us/sample - loss: 79.8471 - val_loss: 11486.4785\n",
      "Epoch 764/1000\n",
      "80/80 [==============================] - 0s 15us/sample - loss: 79.8296 - val_loss: 11486.0234\n",
      "Epoch 765/1000\n",
      "80/80 [==============================] - 0s 19us/sample - loss: 79.8067 - val_loss: 11485.5732\n",
      "Epoch 766/1000\n",
      "80/80 [==============================] - 0s 17us/sample - loss: 79.7901 - val_loss: 11485.1133\n",
      "Epoch 767/1000\n",
      "80/80 [==============================] - 0s 19us/sample - loss: 79.7792 - val_loss: 11484.6348\n",
      "Epoch 768/1000\n",
      "80/80 [==============================] - 0s 17us/sample - loss: 79.7540 - val_loss: 11484.2051\n",
      "Epoch 769/1000\n",
      "80/80 [==============================] - 0s 18us/sample - loss: 79.7432 - val_loss: 11483.6865\n",
      "Epoch 770/1000\n",
      "80/80 [==============================] - 0s 23us/sample - loss: 79.7247 - val_loss: 11483.1641\n",
      "Epoch 771/1000\n",
      "80/80 [==============================] - 0s 19us/sample - loss: 79.7070 - val_loss: 11482.6855\n",
      "Epoch 772/1000\n",
      "80/80 [==============================] - 0s 19us/sample - loss: 79.6899 - val_loss: 11482.2188\n",
      "Epoch 773/1000\n",
      "80/80 [==============================] - 0s 16us/sample - loss: 79.6735 - val_loss: 11481.7168\n",
      "Epoch 774/1000\n",
      "80/80 [==============================] - 0s 18us/sample - loss: 79.6516 - val_loss: 11481.2598\n",
      "Epoch 775/1000\n",
      "80/80 [==============================] - 0s 18us/sample - loss: 79.6356 - val_loss: 11480.7529\n",
      "Epoch 776/1000\n",
      "80/80 [==============================] - 0s 16us/sample - loss: 79.6252 - val_loss: 11480.2012\n",
      "Epoch 777/1000\n",
      "80/80 [==============================] - 0s 16us/sample - loss: 79.6011 - val_loss: 11479.6738\n",
      "Epoch 778/1000\n",
      "80/80 [==============================] - 0s 16us/sample - loss: 79.5841 - val_loss: 11479.1377\n",
      "Epoch 779/1000\n",
      "80/80 [==============================] - 0s 19us/sample - loss: 79.5659 - val_loss: 11478.5840\n",
      "Epoch 780/1000\n",
      "80/80 [==============================] - 0s 16us/sample - loss: 79.5469 - val_loss: 11478.0127\n",
      "Epoch 781/1000\n",
      "80/80 [==============================] - 0s 16us/sample - loss: 79.5358 - val_loss: 11477.4199\n",
      "Epoch 782/1000\n",
      "80/80 [==============================] - 0s 18us/sample - loss: 79.5161 - val_loss: 11476.8828\n",
      "Epoch 783/1000\n",
      "80/80 [==============================] - 0s 20us/sample - loss: 79.4908 - val_loss: 11476.3721\n",
      "Epoch 784/1000\n",
      "80/80 [==============================] - 0s 23us/sample - loss: 79.4805 - val_loss: 11475.7939\n",
      "Epoch 785/1000\n",
      "80/80 [==============================] - 0s 23us/sample - loss: 79.4555 - val_loss: 11475.2529\n",
      "Epoch 786/1000\n",
      "80/80 [==============================] - 0s 19us/sample - loss: 79.4451 - val_loss: 11474.7041\n",
      "Epoch 787/1000\n",
      "80/80 [==============================] - 0s 16us/sample - loss: 79.4272 - val_loss: 11474.1934\n",
      "Epoch 788/1000\n",
      "80/80 [==============================] - 0s 19us/sample - loss: 79.4098 - val_loss: 11473.7002\n",
      "Epoch 789/1000\n",
      "80/80 [==============================] - 0s 16us/sample - loss: 79.3874 - val_loss: 11473.2627\n",
      "Epoch 790/1000\n",
      "80/80 [==============================] - 0s 237us/sample - loss: 79.3711 - val_loss: 11472.7871\n",
      "Epoch 791/1000\n",
      "80/80 [==============================] - 0s 34us/sample - loss: 79.3544 - val_loss: 11472.2734\n",
      "Epoch 792/1000\n",
      "80/80 [==============================] - 0s 51us/sample - loss: 79.3440 - val_loss: 11471.6924\n",
      "Epoch 793/1000\n",
      "80/80 [==============================] - 0s 48us/sample - loss: 79.3186 - val_loss: 11471.1807\n",
      "Epoch 794/1000\n",
      "80/80 [==============================] - 0s 23us/sample - loss: 79.3085 - val_loss: 11470.5908\n",
      "Epoch 795/1000\n",
      "80/80 [==============================] - 0s 32us/sample - loss: 79.2906 - val_loss: 11470.0273\n",
      "Epoch 796/1000\n",
      "80/80 [==============================] - 0s 36us/sample - loss: 79.2734 - val_loss: 11469.5176\n",
      "Epoch 797/1000\n",
      "80/80 [==============================] - 0s 23us/sample - loss: 79.2511 - val_loss: 11469.0498\n",
      "Epoch 798/1000\n",
      "80/80 [==============================] - 0s 25us/sample - loss: 79.2350 - val_loss: 11468.5479\n",
      "Epoch 799/1000\n",
      "80/80 [==============================] - 0s 19us/sample - loss: 79.2253 - val_loss: 11467.9971\n",
      "Epoch 800/1000\n",
      "80/80 [==============================] - 0s 18us/sample - loss: 79.2076 - val_loss: 11467.5059\n",
      "Epoch 801/1000\n",
      "80/80 [==============================] - 0s 19us/sample - loss: 79.1846 - val_loss: 11467.0371\n",
      "Epoch 802/1000\n",
      "80/80 [==============================] - 0s 19us/sample - loss: 79.1756 - val_loss: 11466.5020\n",
      "Epoch 803/1000\n",
      "80/80 [==============================] - 0s 29us/sample - loss: 79.1586 - val_loss: 11466.0254\n",
      "Epoch 804/1000\n",
      "80/80 [==============================] - 0s 19us/sample - loss: 79.1425 - val_loss: 11465.5615\n",
      "Epoch 805/1000\n",
      "80/80 [==============================] - 0s 21us/sample - loss: 79.1212 - val_loss: 11465.1338\n",
      "Epoch 806/1000\n",
      "80/80 [==============================] - 0s 19us/sample - loss: 79.1122 - val_loss: 11464.6514\n",
      "Epoch 807/1000\n",
      "80/80 [==============================] - 0s 27us/sample - loss: 79.0963 - val_loss: 11464.1660\n",
      "Epoch 808/1000\n",
      "80/80 [==============================] - 0s 21us/sample - loss: 79.0810 - val_loss: 11463.6973\n",
      "Epoch 809/1000\n",
      "80/80 [==============================] - 0s 20us/sample - loss: 79.0605 - val_loss: 11463.2510\n",
      "Epoch 810/1000\n",
      "80/80 [==============================] - 0s 16us/sample - loss: 79.0516 - val_loss: 11462.7578\n",
      "Epoch 811/1000\n",
      "80/80 [==============================] - 0s 15us/sample - loss: 79.0360 - val_loss: 11462.2832\n",
      "Epoch 812/1000\n",
      "80/80 [==============================] - 0s 14us/sample - loss: 79.0153 - val_loss: 11461.7949\n",
      "Epoch 813/1000\n",
      "80/80 [==============================] - 0s 16us/sample - loss: 79.0065 - val_loss: 11461.2676\n",
      "Epoch 814/1000\n",
      "80/80 [==============================] - 0s 17us/sample - loss: 78.9849 - val_loss: 11460.7676\n",
      "Epoch 815/1000\n",
      "80/80 [==============================] - 0s 16us/sample - loss: 78.9760 - val_loss: 11460.2227\n",
      "Epoch 816/1000\n",
      "80/80 [==============================] - 0s 16us/sample - loss: 78.9535 - val_loss: 11459.7402\n",
      "Epoch 817/1000\n",
      "80/80 [==============================] - 0s 22us/sample - loss: 78.9447 - val_loss: 11459.1924\n",
      "Epoch 818/1000\n",
      "80/80 [==============================] - 0s 17us/sample - loss: 78.9282 - val_loss: 11458.6895\n",
      "Epoch 819/1000\n",
      "80/80 [==============================] - 0s 21us/sample - loss: 78.9125 - val_loss: 11458.2217\n",
      "Epoch 820/1000\n",
      "80/80 [==============================] - 0s 16us/sample - loss: 78.8974 - val_loss: 11457.7754\n",
      "Epoch 821/1000\n",
      "80/80 [==============================] - 0s 17us/sample - loss: 78.8827 - val_loss: 11457.3447\n",
      "Epoch 822/1000\n",
      "80/80 [==============================] - 0s 16us/sample - loss: 78.8686 - val_loss: 11456.9189\n",
      "Epoch 823/1000\n",
      "80/80 [==============================] - 0s 16us/sample - loss: 78.8549 - val_loss: 11456.5078\n",
      "Epoch 824/1000\n",
      "80/80 [==============================] - 0s 16us/sample - loss: 78.8412 - val_loss: 11456.0986\n",
      "Epoch 825/1000\n",
      "80/80 [==============================] - 0s 16us/sample - loss: 78.8225 - val_loss: 11455.7188\n",
      "Epoch 826/1000\n",
      "80/80 [==============================] - 0s 17us/sample - loss: 78.8148 - val_loss: 11455.2607\n",
      "Epoch 827/1000\n",
      "80/80 [==============================] - 0s 16us/sample - loss: 78.7946 - val_loss: 11454.8223\n",
      "Epoch 828/1000\n",
      "80/80 [==============================] - 0s 18us/sample - loss: 78.7864 - val_loss: 11454.3506\n",
      "Epoch 829/1000\n",
      "80/80 [==============================] - 0s 17us/sample - loss: 78.7710 - val_loss: 11453.8857\n",
      "Epoch 830/1000\n",
      "80/80 [==============================] - 0s 18us/sample - loss: 78.7560 - val_loss: 11453.4395\n",
      "Epoch 831/1000\n",
      "80/80 [==============================] - 0s 17us/sample - loss: 78.7417 - val_loss: 11453.0127\n",
      "Epoch 832/1000\n",
      "80/80 [==============================] - 0s 23us/sample - loss: 78.7224 - val_loss: 11452.5938\n",
      "Epoch 833/1000\n",
      "80/80 [==============================] - 0s 18us/sample - loss: 78.7144 - val_loss: 11452.1270\n",
      "Epoch 834/1000\n",
      "80/80 [==============================] - 0s 26us/sample - loss: 78.6994 - val_loss: 11451.6699\n",
      "Epoch 835/1000\n",
      "80/80 [==============================] - 0s 29us/sample - loss: 78.6848 - val_loss: 11451.2324\n",
      "Epoch 836/1000\n",
      "80/80 [==============================] - 0s 79us/sample - loss: 78.6653 - val_loss: 11450.8066\n",
      "Epoch 837/1000\n",
      "80/80 [==============================] - 0s 151us/sample - loss: 78.6574 - val_loss: 11450.3350\n",
      "Epoch 838/1000\n",
      "80/80 [==============================] - 0s 22us/sample - loss: 78.6425 - val_loss: 11449.8848\n",
      "Epoch 839/1000\n",
      "80/80 [==============================] - 0s 20us/sample - loss: 78.6280 - val_loss: 11449.4668\n",
      "Epoch 840/1000\n",
      "80/80 [==============================] - 0s 60us/sample - loss: 78.6140 - val_loss: 11449.0439\n",
      "Epoch 841/1000\n",
      "80/80 [==============================] - 0s 34us/sample - loss: 78.6002 - val_loss: 11448.6436\n",
      "Epoch 842/1000\n",
      "80/80 [==============================] - 0s 18us/sample - loss: 78.5815 - val_loss: 11448.2578\n",
      "Epoch 843/1000\n",
      "80/80 [==============================] - 0s 19us/sample - loss: 78.5738 - val_loss: 11447.8145\n",
      "Epoch 844/1000\n",
      "80/80 [==============================] - 0s 18us/sample - loss: 78.5534 - val_loss: 11447.3848\n",
      "Epoch 845/1000\n",
      "80/80 [==============================] - 0s 16us/sample - loss: 78.5455 - val_loss: 11446.8818\n",
      "Epoch 846/1000\n",
      "80/80 [==============================] - 0s 16us/sample - loss: 78.5302 - val_loss: 11446.4121\n",
      "Epoch 847/1000\n",
      "80/80 [==============================] - 0s 18us/sample - loss: 78.5155 - val_loss: 11445.9697\n",
      "Epoch 848/1000\n",
      "80/80 [==============================] - 0s 22us/sample - loss: 78.4955 - val_loss: 11445.5547\n",
      "Epoch 849/1000\n",
      "80/80 [==============================] - 0s 22us/sample - loss: 78.4876 - val_loss: 11445.0996\n",
      "Epoch 850/1000\n",
      "80/80 [==============================] - 0s 23us/sample - loss: 78.4666 - val_loss: 11444.6543\n",
      "Epoch 851/1000\n",
      "80/80 [==============================] - 0s 22us/sample - loss: 78.4583 - val_loss: 11444.1699\n",
      "Epoch 852/1000\n",
      "80/80 [==============================] - 0s 16us/sample - loss: 78.4365 - val_loss: 11443.7061\n",
      "Epoch 853/1000\n",
      "80/80 [==============================] - 0s 17us/sample - loss: 78.4213 - val_loss: 11443.1953\n",
      "Epoch 854/1000\n",
      "80/80 [==============================] - 0s 16us/sample - loss: 78.4125 - val_loss: 11442.6484\n",
      "Epoch 855/1000\n",
      "80/80 [==============================] - 0s 15us/sample - loss: 78.3958 - val_loss: 11442.1357\n",
      "Epoch 856/1000\n",
      "80/80 [==============================] - 0s 16us/sample - loss: 78.3799 - val_loss: 11441.6533\n",
      "Epoch 857/1000\n",
      "80/80 [==============================] - 0s 19us/sample - loss: 78.3648 - val_loss: 11441.2158\n",
      "Epoch 858/1000\n",
      "80/80 [==============================] - 0s 17us/sample - loss: 78.3502 - val_loss: 11440.7949\n",
      "Epoch 859/1000\n",
      "80/80 [==============================] - 0s 17us/sample - loss: 78.3361 - val_loss: 11440.3672\n",
      "Epoch 860/1000\n",
      "80/80 [==============================] - 0s 18us/sample - loss: 78.3224 - val_loss: 11439.9531\n",
      "Epoch 861/1000\n",
      "80/80 [==============================] - 0s 18us/sample - loss: 78.3089 - val_loss: 11439.5527\n",
      "Epoch 862/1000\n",
      "80/80 [==============================] - 0s 17us/sample - loss: 78.2904 - val_loss: 11439.1592\n",
      "Epoch 863/1000\n",
      "80/80 [==============================] - 0s 17us/sample - loss: 78.2831 - val_loss: 11438.6826\n",
      "Epoch 864/1000\n",
      "80/80 [==============================] - 0s 16us/sample - loss: 78.2629 - val_loss: 11438.2461\n",
      "Epoch 865/1000\n",
      "80/80 [==============================] - 0s 16us/sample - loss: 78.2551 - val_loss: 11437.7666\n",
      "Epoch 866/1000\n",
      "80/80 [==============================] - 0s 19us/sample - loss: 78.2400 - val_loss: 11437.2871\n",
      "Epoch 867/1000\n",
      "80/80 [==============================] - 0s 19us/sample - loss: 78.2195 - val_loss: 11436.8535\n",
      "Epoch 868/1000\n",
      "80/80 [==============================] - 0s 14us/sample - loss: 78.2116 - val_loss: 11436.3672\n",
      "Epoch 869/1000\n",
      "80/80 [==============================] - 0s 14us/sample - loss: 78.1964 - val_loss: 11435.8955\n",
      "Epoch 870/1000\n",
      "80/80 [==============================] - 0s 15us/sample - loss: 78.1818 - val_loss: 11435.4365\n",
      "Epoch 871/1000\n",
      "80/80 [==============================] - 0s 15us/sample - loss: 78.1676 - val_loss: 11435.0254\n",
      "Epoch 872/1000\n",
      "80/80 [==============================] - 0s 14us/sample - loss: 78.1539 - val_loss: 11434.6143\n",
      "Epoch 873/1000\n",
      "80/80 [==============================] - 0s 18us/sample - loss: 78.1350 - val_loss: 11434.2188\n",
      "Epoch 874/1000\n",
      "80/80 [==============================] - 0s 15us/sample - loss: 78.1275 - val_loss: 11433.7812\n",
      "Epoch 875/1000\n",
      "80/80 [==============================] - 0s 15us/sample - loss: 78.1071 - val_loss: 11433.3623\n",
      "Epoch 876/1000\n",
      "80/80 [==============================] - 0s 15us/sample - loss: 78.0992 - val_loss: 11432.8867\n",
      "Epoch 877/1000\n",
      "80/80 [==============================] - 0s 15us/sample - loss: 78.0778 - val_loss: 11432.4336\n",
      "Epoch 878/1000\n",
      "80/80 [==============================] - 0s 15us/sample - loss: 78.0698 - val_loss: 11431.9443\n",
      "Epoch 879/1000\n",
      "80/80 [==============================] - 0s 15us/sample - loss: 78.0540 - val_loss: 11431.4717\n",
      "Epoch 880/1000\n",
      "80/80 [==============================] - 0s 15us/sample - loss: 78.0390 - val_loss: 11431.0010\n",
      "Epoch 881/1000\n",
      "80/80 [==============================] - 0s 15us/sample - loss: 78.0247 - val_loss: 11430.5605\n",
      "Epoch 882/1000\n",
      "80/80 [==============================] - 0s 15us/sample - loss: 78.0107 - val_loss: 11430.1504\n",
      "Epoch 883/1000\n",
      "80/80 [==============================] - 0s 17us/sample - loss: 77.9915 - val_loss: 11429.7559\n",
      "Epoch 884/1000\n",
      "80/80 [==============================] - 0s 23us/sample - loss: 77.9777 - val_loss: 11429.3223\n",
      "Epoch 885/1000\n",
      "80/80 [==============================] - 0s 16us/sample - loss: 77.9628 - val_loss: 11428.8330\n",
      "Epoch 886/1000\n",
      "80/80 [==============================] - 0s 16us/sample - loss: 77.9470 - val_loss: 11428.3184\n",
      "Epoch 887/1000\n",
      "80/80 [==============================] - 0s 17us/sample - loss: 77.9381 - val_loss: 11427.7686\n",
      "Epoch 888/1000\n",
      "80/80 [==============================] - 0s 17us/sample - loss: 77.9210 - val_loss: 11427.2598\n",
      "Epoch 889/1000\n",
      "80/80 [==============================] - 0s 15us/sample - loss: 77.8980 - val_loss: 11426.8018\n",
      "Epoch 890/1000\n",
      "80/80 [==============================] - 0s 15us/sample - loss: 77.8896 - val_loss: 11426.3135\n",
      "Epoch 891/1000\n",
      "80/80 [==============================] - 0s 15us/sample - loss: 77.8664 - val_loss: 11425.8730\n",
      "Epoch 892/1000\n",
      "80/80 [==============================] - 0s 15us/sample - loss: 77.8580 - val_loss: 11425.3906\n",
      "Epoch 893/1000\n",
      "80/80 [==============================] - 0s 14us/sample - loss: 77.8416 - val_loss: 11424.9512\n",
      "Epoch 894/1000\n",
      "80/80 [==============================] - 0s 14us/sample - loss: 77.8261 - val_loss: 11424.5312\n",
      "Epoch 895/1000\n",
      "80/80 [==============================] - 0s 14us/sample - loss: 77.8050 - val_loss: 11424.1455\n",
      "Epoch 896/1000\n",
      "80/80 [==============================] - 0s 18us/sample - loss: 77.7972 - val_loss: 11423.7080\n",
      "Epoch 897/1000\n",
      "80/80 [==============================] - 0s 15us/sample - loss: 77.7818 - val_loss: 11423.2891\n",
      "Epoch 898/1000\n",
      "80/80 [==============================] - 0s 15us/sample - loss: 77.7670 - val_loss: 11422.8877\n",
      "Epoch 899/1000\n",
      "80/80 [==============================] - 0s 15us/sample - loss: 77.7527 - val_loss: 11422.4922\n",
      "Epoch 900/1000\n",
      "80/80 [==============================] - 0s 15us/sample - loss: 77.7333 - val_loss: 11422.1104\n",
      "Epoch 901/1000\n",
      "80/80 [==============================] - 0s 14us/sample - loss: 77.7194 - val_loss: 11421.6826\n",
      "Epoch 902/1000\n",
      "80/80 [==============================] - 0s 15us/sample - loss: 77.7044 - val_loss: 11421.2158\n",
      "Epoch 903/1000\n",
      "80/80 [==============================] - 0s 14us/sample - loss: 77.6884 - val_loss: 11420.7217\n",
      "Epoch 904/1000\n",
      "80/80 [==============================] - 0s 16us/sample - loss: 77.6796 - val_loss: 11420.1855\n",
      "Epoch 905/1000\n",
      "80/80 [==============================] - 0s 25us/sample - loss: 77.6625 - val_loss: 11419.6875\n",
      "Epoch 906/1000\n",
      "80/80 [==============================] - 0s 16us/sample - loss: 77.6395 - val_loss: 11419.2490\n",
      "Epoch 907/1000\n",
      "80/80 [==============================] - 0s 18us/sample - loss: 77.6239 - val_loss: 11418.7930\n",
      "Epoch 908/1000\n",
      "80/80 [==============================] - 0s 16us/sample - loss: 77.6151 - val_loss: 11418.3076\n",
      "Epoch 909/1000\n",
      "80/80 [==============================] - 0s 18us/sample - loss: 77.5909 - val_loss: 11417.8750\n",
      "Epoch 910/1000\n",
      "80/80 [==============================] - 0s 16us/sample - loss: 77.5823 - val_loss: 11417.4092\n",
      "Epoch 911/1000\n",
      "80/80 [==============================] - 0s 15us/sample - loss: 77.5655 - val_loss: 11416.9609\n",
      "Epoch 912/1000\n",
      "80/80 [==============================] - 0s 17us/sample - loss: 77.5496 - val_loss: 11416.5449\n",
      "Epoch 913/1000\n",
      "80/80 [==============================] - 0s 16us/sample - loss: 77.5344 - val_loss: 11416.1475\n",
      "Epoch 914/1000\n",
      "80/80 [==============================] - 0s 18us/sample - loss: 77.5139 - val_loss: 11415.7676\n",
      "Epoch 915/1000\n",
      "80/80 [==============================] - 0s 16us/sample - loss: 77.4995 - val_loss: 11415.3350\n",
      "Epoch 916/1000\n",
      "80/80 [==============================] - 0s 18us/sample - loss: 77.4913 - val_loss: 11414.8438\n",
      "Epoch 917/1000\n",
      "80/80 [==============================] - 0s 17us/sample - loss: 77.4751 - val_loss: 11414.4033\n",
      "Epoch 918/1000\n",
      "80/80 [==============================] - 0s 17us/sample - loss: 77.4597 - val_loss: 11413.9824\n",
      "Epoch 919/1000\n",
      "80/80 [==============================] - 0s 18us/sample - loss: 77.4450 - val_loss: 11413.5762\n",
      "Epoch 920/1000\n",
      "80/80 [==============================] - 0s 17us/sample - loss: 77.4307 - val_loss: 11413.1934\n",
      "Epoch 921/1000\n",
      "80/80 [==============================] - 0s 19us/sample - loss: 77.4170 - val_loss: 11412.8125\n",
      "Epoch 922/1000\n",
      "80/80 [==============================] - 0s 20us/sample - loss: 77.4036 - val_loss: 11412.4326\n",
      "Epoch 923/1000\n",
      "80/80 [==============================] - 0s 20us/sample - loss: 77.3904 - val_loss: 11412.0723\n",
      "Epoch 924/1000\n",
      "80/80 [==============================] - 0s 19us/sample - loss: 77.3722 - val_loss: 11411.7090\n",
      "Epoch 925/1000\n",
      "80/80 [==============================] - 0s 20us/sample - loss: 77.3650 - val_loss: 11411.2852\n",
      "Epoch 926/1000\n",
      "80/80 [==============================] - 0s 19us/sample - loss: 77.3509 - val_loss: 11410.8711\n",
      "Epoch 927/1000\n",
      "80/80 [==============================] - 0s 19us/sample - loss: 77.3371 - val_loss: 11410.4795\n",
      "Epoch 928/1000\n",
      "80/80 [==============================] - 0s 23us/sample - loss: 77.3181 - val_loss: 11410.1113\n",
      "Epoch 929/1000\n",
      "80/80 [==============================] - 0s 257us/sample - loss: 77.3108 - val_loss: 11409.6768\n",
      "Epoch 930/1000\n",
      "80/80 [==============================] - 0s 129us/sample - loss: 77.2964 - val_loss: 11409.2529\n",
      "Epoch 931/1000\n",
      "80/80 [==============================] - 0s 31us/sample - loss: 77.2764 - val_loss: 11408.8701\n",
      "Epoch 932/1000\n",
      "80/80 [==============================] - 0s 60us/sample - loss: 77.2690 - val_loss: 11408.4395\n",
      "Epoch 933/1000\n",
      "80/80 [==============================] - 0s 39us/sample - loss: 77.2479 - val_loss: 11408.0293\n",
      "Epoch 934/1000\n",
      "80/80 [==============================] - 0s 27us/sample - loss: 77.2401 - val_loss: 11407.5732\n",
      "Epoch 935/1000\n",
      "80/80 [==============================] - 0s 39us/sample - loss: 77.2181 - val_loss: 11407.1602\n",
      "Epoch 936/1000\n",
      "80/80 [==============================] - 0s 20us/sample - loss: 77.2101 - val_loss: 11406.6982\n",
      "Epoch 937/1000\n",
      "80/80 [==============================] - 0s 20us/sample - loss: 77.1876 - val_loss: 11406.2598\n",
      "Epoch 938/1000\n",
      "80/80 [==============================] - 0s 19us/sample - loss: 77.1795 - val_loss: 11405.7715\n",
      "Epoch 939/1000\n",
      "80/80 [==============================] - 0s 19us/sample - loss: 77.1566 - val_loss: 11405.3232\n",
      "Epoch 940/1000\n",
      "80/80 [==============================] - 0s 16us/sample - loss: 77.1484 - val_loss: 11404.8418\n",
      "Epoch 941/1000\n",
      "80/80 [==============================] - 0s 15us/sample - loss: 77.1253 - val_loss: 11404.4082\n",
      "Epoch 942/1000\n",
      "80/80 [==============================] - 0s 15us/sample - loss: 77.1096 - val_loss: 11403.9355\n",
      "Epoch 943/1000\n",
      "80/80 [==============================] - 0s 17us/sample - loss: 77.0932 - val_loss: 11403.4219\n",
      "Epoch 944/1000\n",
      "80/80 [==============================] - 0s 23us/sample - loss: 77.0761 - val_loss: 11402.8887\n",
      "Epoch 945/1000\n",
      "80/80 [==============================] - 0s 23us/sample - loss: 77.0669 - val_loss: 11402.3311\n",
      "Epoch 946/1000\n",
      "80/80 [==============================] - 0s 17us/sample - loss: 77.0487 - val_loss: 11401.8340\n",
      "Epoch 947/1000\n",
      "80/80 [==============================] - 0s 19us/sample - loss: 77.0317 - val_loss: 11401.3740\n",
      "Epoch 948/1000\n",
      "80/80 [==============================] - 0s 20us/sample - loss: 77.0089 - val_loss: 11400.9521\n",
      "Epoch 949/1000\n",
      "80/80 [==============================] - 0s 24us/sample - loss: 76.9934 - val_loss: 11400.4785\n",
      "Epoch 950/1000\n",
      "80/80 [==============================] - 0s 19us/sample - loss: 76.9848 - val_loss: 11399.9658\n",
      "Epoch 951/1000\n",
      "80/80 [==============================] - 0s 15us/sample - loss: 76.9677 - val_loss: 11399.4805\n",
      "Epoch 952/1000\n",
      "80/80 [==============================] - 0s 16us/sample - loss: 76.9517 - val_loss: 11399.0332\n",
      "Epoch 953/1000\n",
      "80/80 [==============================] - 0s 16us/sample - loss: 76.9364 - val_loss: 11398.6074\n",
      "Epoch 954/1000\n",
      "80/80 [==============================] - 0s 16us/sample - loss: 76.9157 - val_loss: 11398.2246\n",
      "Epoch 955/1000\n",
      "80/80 [==============================] - 0s 17us/sample - loss: 76.9011 - val_loss: 11397.7861\n",
      "Epoch 956/1000\n",
      "80/80 [==============================] - 0s 16us/sample - loss: 76.8929 - val_loss: 11397.3027\n",
      "Epoch 957/1000\n",
      "80/80 [==============================] - 0s 17us/sample - loss: 76.8766 - val_loss: 11396.8516\n",
      "Epoch 958/1000\n",
      "80/80 [==============================] - 0s 16us/sample - loss: 76.8611 - val_loss: 11396.4404\n",
      "Epoch 959/1000\n",
      "80/80 [==============================] - 0s 19us/sample - loss: 76.8463 - val_loss: 11396.0449\n",
      "Epoch 960/1000\n",
      "80/80 [==============================] - 0s 17us/sample - loss: 76.8260 - val_loss: 11395.6709\n",
      "Epoch 961/1000\n",
      "80/80 [==============================] - 0s 21us/sample - loss: 76.8184 - val_loss: 11395.2441\n",
      "Epoch 962/1000\n",
      "80/80 [==============================] - 0s 30us/sample - loss: 76.8033 - val_loss: 11394.8223\n",
      "Epoch 963/1000\n",
      "80/80 [==============================] - 0s 29us/sample - loss: 76.7826 - val_loss: 11394.4277\n",
      "Epoch 964/1000\n",
      "80/80 [==============================] - 0s 24us/sample - loss: 76.7682 - val_loss: 11393.9785\n",
      "Epoch 965/1000\n",
      "80/80 [==============================] - 0s 43us/sample - loss: 76.7600 - val_loss: 11393.4736\n",
      "Epoch 966/1000\n",
      "80/80 [==============================] - 0s 18us/sample - loss: 76.7369 - val_loss: 11393.0098\n",
      "Epoch 967/1000\n",
      "80/80 [==============================] - 0s 20us/sample - loss: 76.7212 - val_loss: 11392.5088\n",
      "Epoch 968/1000\n",
      "80/80 [==============================] - 0s 19us/sample - loss: 76.7126 - val_loss: 11391.9688\n",
      "Epoch 969/1000\n",
      "80/80 [==============================] - 0s 23us/sample - loss: 76.6954 - val_loss: 11391.4746\n",
      "Epoch 970/1000\n",
      "80/80 [==============================] - 0s 56us/sample - loss: 76.6792 - val_loss: 11391.0137\n",
      "Epoch 971/1000\n",
      "80/80 [==============================] - 0s 26us/sample - loss: 76.6638 - val_loss: 11390.5840\n",
      "Epoch 972/1000\n",
      "80/80 [==============================] - 0s 57us/sample - loss: 76.6428 - val_loss: 11390.1855\n",
      "Epoch 973/1000\n",
      "80/80 [==============================] - 0s 75us/sample - loss: 76.6350 - val_loss: 11389.7363\n",
      "Epoch 974/1000\n",
      "80/80 [==============================] - 0s 20us/sample - loss: 76.6130 - val_loss: 11389.3330\n",
      "Epoch 975/1000\n",
      "80/80 [==============================] - 0s 17us/sample - loss: 76.6050 - val_loss: 11388.8682\n",
      "Epoch 976/1000\n",
      "80/80 [==============================] - 0s 20us/sample - loss: 76.5824 - val_loss: 11388.4463\n",
      "Epoch 977/1000\n",
      "80/80 [==============================] - 0s 17us/sample - loss: 76.5742 - val_loss: 11387.9717\n",
      "Epoch 978/1000\n",
      "80/80 [==============================] - 0s 18us/sample - loss: 76.5581 - val_loss: 11387.5449\n",
      "Epoch 979/1000\n",
      "80/80 [==============================] - 0s 31us/sample - loss: 76.5361 - val_loss: 11387.1455\n",
      "Epoch 980/1000\n",
      "80/80 [==============================] - 0s 49us/sample - loss: 76.5210 - val_loss: 11386.7100\n",
      "Epoch 981/1000\n",
      "80/80 [==============================] - 0s 26us/sample - loss: 76.5049 - val_loss: 11386.2383\n",
      "Epoch 982/1000\n",
      "80/80 [==============================] - 0s 24us/sample - loss: 76.4881 - val_loss: 11385.7402\n",
      "Epoch 983/1000\n",
      "80/80 [==============================] - 0s 18us/sample - loss: 76.4790 - val_loss: 11385.2119\n",
      "Epoch 984/1000\n",
      "80/80 [==============================] - 0s 20us/sample - loss: 76.4609 - val_loss: 11384.7363\n",
      "Epoch 985/1000\n",
      "80/80 [==============================] - 0s 19us/sample - loss: 76.4439 - val_loss: 11384.2949\n",
      "Epoch 986/1000\n",
      "80/80 [==============================] - 0s 44us/sample - loss: 76.4211 - val_loss: 11383.9141\n",
      "Epoch 987/1000\n",
      "80/80 [==============================] - 0s 23us/sample - loss: 76.4128 - val_loss: 11383.4668\n",
      "Epoch 988/1000\n",
      "80/80 [==============================] - 0s 27us/sample - loss: 76.3965 - val_loss: 11383.0449\n",
      "Epoch 989/1000\n",
      "80/80 [==============================] - 0s 45us/sample - loss: 76.3745 - val_loss: 11382.6514\n",
      "Epoch 990/1000\n",
      "80/80 [==============================] - 0s 38us/sample - loss: 76.3664 - val_loss: 11382.2012\n",
      "Epoch 991/1000\n",
      "80/80 [==============================] - 0s 19us/sample - loss: 76.3504 - val_loss: 11381.7686\n",
      "Epoch 992/1000\n",
      "80/80 [==============================] - 0s 18us/sample - loss: 76.3351 - val_loss: 11381.3467\n",
      "Epoch 993/1000\n",
      "80/80 [==============================] - 0s 18us/sample - loss: 76.3142 - val_loss: 11380.9629\n",
      "Epoch 994/1000\n",
      "80/80 [==============================] - 0s 24us/sample - loss: 76.2996 - val_loss: 11380.5371\n",
      "Epoch 995/1000\n",
      "80/80 [==============================] - 0s 37us/sample - loss: 76.2839 - val_loss: 11380.0723\n",
      "Epoch 996/1000\n",
      "80/80 [==============================] - 0s 31us/sample - loss: 76.2752 - val_loss: 11379.5820\n",
      "Epoch 997/1000\n",
      "80/80 [==============================] - 0s 19us/sample - loss: 76.2580 - val_loss: 11379.1279\n",
      "Epoch 998/1000\n",
      "80/80 [==============================] - 0s 23us/sample - loss: 76.2417 - val_loss: 11378.7139\n",
      "Epoch 999/1000\n",
      "80/80 [==============================] - 0s 24us/sample - loss: 76.2196 - val_loss: 11378.3350\n",
      "Epoch 1000/1000\n",
      "80/80 [==============================] - 0s 25us/sample - loss: 76.2115 - val_loss: 11377.9082\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x29c819c90>]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainig = model.fit(x, y, epochs=1000, batch_size=50, validation_split=0.2)\n",
    "\n",
    "plt.plot(trainig.epoch, trainig.history['loss'])\n",
    "plt.plot(trainig.epoch, trainig.history['val_loss'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, x , y = PrepareData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size = 0.1, random_state = 42)\n",
    "\n",
    "X_train = torch.tensor(X_train.values, dtype = torch.float32)\n",
    "X_test = torch.tensor(X_test.values, dtype = torch.float32)\n",
    "y_train = torch.tensor(y_train.values, dtype = torch.float32)\n",
    "y_test = torch.tensor(y_test.values, dtype = torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape=[None, 100])\n",
    "Y = tf.placeholder(tf.float32, shape=y_train.shape)\n",
    "\n",
    "W_1 = tf.Variable(tf.truncated_normal([100, 10], stddev=0.01))\n",
    "b_1 = tf.Variable(tf.zeros([10]))\n",
    "h_1 = tf.nn.relu(tf.matmul(X, W_1) + b_1)\n",
    "\n",
    "W_2 = tf.Variable(tf.truncated_normal([10, 5], stddev=0.01))\n",
    "b_2 = tf.Variable(tf.zeros([5]))\n",
    "h_2 = tf.nn.relu(tf.matmul(h_1, W_2) + b_2)\n",
    "\n",
    "W_3 = tf.Variable(tf.truncated_normal([5, 3], stddev=0.01))\n",
    "b_3 = tf.Variable(tf.zeros([3]))\n",
    "h_3 = tf.nn.relu(tf.matmul(h_2, W_3) + b_3)\n",
    "\n",
    "W_out = tf.Variable(tf.truncated_normal([3, 1], stddev=0.01))\n",
    "b_out = tf.Variable(tf.zeros([1]))\n",
    "y_pred = tf.identity(tf.add(tf.matmul(h_3, W_out), b_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = tf.reduce_mean(tf.square(Y - y_pred))\n",
    "minimization_op = tf.train.AdamOptimizer(learning_rate=0.01)\n",
    "train = minimization_op.minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "feed_dict = {\n",
    "    X: X_train,\n",
    "    Y: y_train\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for epoch in range(1000):\n",
    "            c = sess.run(cost, feed_dict)\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(\"Epoch:\", epoch, \" Loss:\", c,\" Accuracy:\")\n",
    "    \n",
    "    sess.run(train, feed_dict)\n",
    "    predictions = sess.run([y_pred], feed_dict={X:X_test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0.01033178],\n",
       "        [0.0103295 ],\n",
       "        [0.01033315],\n",
       "        [0.01032928],\n",
       "        [0.0103367 ],\n",
       "        [0.01033142],\n",
       "        [0.01032929],\n",
       "        [0.01033213],\n",
       "        [0.0103284 ],\n",
       "        [0.01033587]], dtype=float32)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 202.7988,    4.6349,  -19.4410,  100.4042,  504.5619, -101.1191,\n",
       "          79.4449,  119.4657,   22.0333,  -90.5688])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, x, y = PrepareData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size = 0.1, random_state = 42)\n",
    "\n",
    "X_train = torch.tensor(X_train.values, dtype = torch.float32)\n",
    "X_test = torch.tensor(X_test.values, dtype = torch.float32)\n",
    "y_train = torch.tensor(y_train.values, dtype = torch.float32)\n",
    "y_test = torch.tensor(y_test.values, dtype = torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class linearRegression(torch.nn.Module): \n",
    "    def __init__(self,input_dim):\n",
    "        super(linearRegression, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(input_dim, 10)\n",
    "        self.fc2 = torch.nn.Linear(10, 5)\n",
    "        self.fc3 = torch.nn.Linear(5, 3)\n",
    "        self.fc4 = torch.nn.Linear(3, 1)\n",
    "\n",
    "    def forward(self,d):\n",
    "        out = torch.torch.relu(self.fc1(d))\n",
    "        out = torch.torch.relu(self.fc2(out)) \n",
    "        out = torch.torch.relu(self.fc3(out))\n",
    "        out = self.fc4(out)\n",
    "        return out      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = X_train.shape[1]\n",
    "torch.manual_seed(42)\n",
    "model = linearRegression(input_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "loss = torch.nn.MSELoss()\n",
    "optimizers = optim.Adam(params = model.parameters(),lr = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch:0]: The loss value for training part=48599.5390625\n",
      "[epoch:10]: The loss value for training part=48593.68359375\n",
      "[epoch:20]: The loss value for training part=48543.97265625\n",
      "[epoch:30]: The loss value for training part=48269.140625\n",
      "[epoch:40]: The loss value for training part=47245.984375\n",
      "[epoch:50]: The loss value for training part=44436.296875\n",
      "[epoch:60]: The loss value for training part=38946.25\n",
      "[epoch:70]: The loss value for training part=33257.859375\n",
      "[epoch:80]: The loss value for training part=31540.09765625\n",
      "[epoch:90]: The loss value for training part=30087.1796875\n",
      "[epoch:100]: The loss value for training part=29298.896484375\n",
      "[epoch:110]: The loss value for training part=28817.5390625\n",
      "[epoch:120]: The loss value for training part=28487.705078125\n",
      "[epoch:130]: The loss value for training part=28282.689453125\n",
      "[epoch:140]: The loss value for training part=28168.34375\n",
      "[epoch:150]: The loss value for training part=28111.376953125\n",
      "[epoch:160]: The loss value for training part=28080.703125\n",
      "[epoch:170]: The loss value for training part=28056.580078125\n",
      "[epoch:180]: The loss value for training part=28032.7109375\n",
      "[epoch:190]: The loss value for training part=28009.482421875\n",
      "[epoch:200]: The loss value for training part=27986.75\n",
      "[epoch:210]: The loss value for training part=27964.341796875\n",
      "[epoch:220]: The loss value for training part=27942.22265625\n",
      "[epoch:230]: The loss value for training part=27920.349609375\n",
      "[epoch:240]: The loss value for training part=27898.697265625\n",
      "[epoch:250]: The loss value for training part=27877.25\n",
      "[epoch:260]: The loss value for training part=27855.974609375\n",
      "[epoch:270]: The loss value for training part=27834.873046875\n",
      "[epoch:280]: The loss value for training part=27813.9140625\n",
      "[epoch:290]: The loss value for training part=27793.099609375\n",
      "[epoch:300]: The loss value for training part=27772.416015625\n",
      "[epoch:310]: The loss value for training part=27751.853515625\n",
      "[epoch:320]: The loss value for training part=27731.39453125\n",
      "[epoch:330]: The loss value for training part=27711.046875\n",
      "[epoch:340]: The loss value for training part=27690.796875\n",
      "[epoch:350]: The loss value for training part=27670.63671875\n",
      "[epoch:360]: The loss value for training part=27650.564453125\n",
      "[epoch:370]: The loss value for training part=27630.578125\n",
      "[epoch:380]: The loss value for training part=27610.666015625\n",
      "[epoch:390]: The loss value for training part=27590.830078125\n",
      "[epoch:400]: The loss value for training part=27571.06640625\n",
      "[epoch:410]: The loss value for training part=27551.369140625\n",
      "[epoch:420]: The loss value for training part=27531.73828125\n",
      "[epoch:430]: The loss value for training part=27512.1640625\n",
      "[epoch:440]: The loss value for training part=27492.65625\n",
      "[epoch:450]: The loss value for training part=27473.203125\n",
      "[epoch:460]: The loss value for training part=27453.802734375\n",
      "[epoch:470]: The loss value for training part=27434.4609375\n",
      "[epoch:480]: The loss value for training part=27415.1640625\n",
      "[epoch:490]: The loss value for training part=27395.921875\n",
      "[epoch:500]: The loss value for training part=27376.72265625\n",
      "[epoch:510]: The loss value for training part=27357.57421875\n",
      "[epoch:520]: The loss value for training part=27338.46875\n",
      "[epoch:530]: The loss value for training part=27319.40625\n",
      "[epoch:540]: The loss value for training part=27300.388671875\n",
      "[epoch:550]: The loss value for training part=27281.41015625\n",
      "[epoch:560]: The loss value for training part=27262.46875\n",
      "[epoch:570]: The loss value for training part=27243.57421875\n",
      "[epoch:580]: The loss value for training part=27224.7109375\n",
      "[epoch:590]: The loss value for training part=27205.888671875\n",
      "[epoch:600]: The loss value for training part=27187.099609375\n",
      "[epoch:610]: The loss value for training part=27168.349609375\n",
      "[epoch:620]: The loss value for training part=27149.6328125\n",
      "[epoch:630]: The loss value for training part=27130.947265625\n",
      "[epoch:640]: The loss value for training part=27112.30078125\n",
      "[epoch:650]: The loss value for training part=27093.68359375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch:660]: The loss value for training part=27075.09765625\n",
      "[epoch:670]: The loss value for training part=27056.541015625\n",
      "[epoch:680]: The loss value for training part=27038.01953125\n",
      "[epoch:690]: The loss value for training part=27019.521484375\n",
      "[epoch:700]: The loss value for training part=27001.060546875\n",
      "[epoch:710]: The loss value for training part=26982.626953125\n",
      "[epoch:720]: The loss value for training part=26964.22265625\n",
      "[epoch:730]: The loss value for training part=26945.84375\n",
      "[epoch:740]: The loss value for training part=26927.494140625\n",
      "[epoch:750]: The loss value for training part=26909.17578125\n",
      "[epoch:760]: The loss value for training part=26890.876953125\n",
      "[epoch:770]: The loss value for training part=26872.607421875\n",
      "[epoch:780]: The loss value for training part=26854.3671875\n",
      "[epoch:790]: The loss value for training part=26836.146484375\n",
      "[epoch:800]: The loss value for training part=26817.9609375\n",
      "[epoch:810]: The loss value for training part=26799.7890625\n",
      "[epoch:820]: The loss value for training part=26781.646484375\n",
      "[epoch:830]: The loss value for training part=26763.52734375\n",
      "[epoch:840]: The loss value for training part=26745.44140625\n",
      "[epoch:850]: The loss value for training part=26727.373046875\n",
      "[epoch:860]: The loss value for training part=26709.32421875\n",
      "[epoch:870]: The loss value for training part=26691.3046875\n",
      "[epoch:880]: The loss value for training part=26673.302734375\n",
      "[epoch:890]: The loss value for training part=26655.328125\n",
      "[epoch:900]: The loss value for training part=26637.375\n",
      "[epoch:910]: The loss value for training part=26619.44140625\n",
      "[epoch:920]: The loss value for training part=26601.533203125\n",
      "[epoch:930]: The loss value for training part=26583.646484375\n",
      "[epoch:940]: The loss value for training part=26565.77734375\n",
      "[epoch:950]: The loss value for training part=26547.93359375\n",
      "[epoch:960]: The loss value for training part=26530.111328125\n",
      "[epoch:970]: The loss value for training part=26512.310546875\n",
      "[epoch:980]: The loss value for training part=26494.53125\n",
      "[epoch:990]: The loss value for training part=26476.771484375\n"
     ]
    }
   ],
   "source": [
    "num_of_epochs = 1000\n",
    "for i in range(num_of_epochs):\n",
    "  y_train_prediction=model(X_train)\n",
    "  loss_value = loss(y_train_prediction.squeeze(), y_train)\n",
    "  optimizers.zero_grad()\n",
    "  loss_value.backward()\n",
    "  optimizers.step()\n",
    "\n",
    "  if i % 10 == 0:\n",
    "    print(f'[epoch:{i}]: The loss value for training part={loss_value}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss value : 74329.9531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mykola/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([10])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "   model.eval()\n",
    "   y_test_prediction = model(X_test[0])\n",
    "   test_loss = loss(y_test_prediction.squeeze(), y_test)\n",
    "   print(f'Test loss value : {test_loss.item():.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DES",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
